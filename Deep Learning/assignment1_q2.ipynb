{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Daniel's copy of Assignment 1 (python) “Bag, not bag”.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcHXpScZ7VaW"
      },
      "source": [
        "##Setup##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iefuZyphipbz"
      },
      "source": [
        "# Mount Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKgXZZ0DhH99"
      },
      "source": [
        "# Import libraries\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xA7W9MdQhhtg"
      },
      "source": [
        "# Load dataset\n",
        "path = \"/content/drive/My Drive/Colab Notebooks/dl_coursework1/data\"\n",
        "\n",
        "trainxs = np.load(f\"{path}/fashion-train-imgs.npz\")\n",
        "trainys = np.load(f\"{path}/fashion-train-labels.npz\")\n",
        "devxs   = np.load(f\"{path}/fashion-dev-imgs.npz\")\n",
        "devys   = np.load(f\"{path}/fashion-dev-labels.npz\")\n",
        "testxs  = np.load(f\"{path}/fashion-test-imgs.npz\")\n",
        "testys = np.load(f\"{path}/fashion-test-labels.npz\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J11cxg097bIK"
      },
      "source": [
        "##Data exploration & preparation##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcT13YoCkBQ-"
      },
      "source": [
        "# Display an example image with label, specify number below\n",
        "\n",
        "examples_to_show = list(range(40,42))\n",
        "\n",
        "for example in examples_to_show:\n",
        "  print(f\"Y Label: {trainys[example]}\")\n",
        "\n",
        "  plt.imshow(trainxs[:, :, example].T, cmap='gray')\n",
        "  plt.axis('off')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RRI9gON8jXB"
      },
      "source": [
        "# Reshape data (flatten images to 1D)\n",
        "\n",
        "def flatten_2D_images(nparray):\n",
        "  return np.reshape(nparray, (nparray.shape[0]*nparray.shape[1], nparray.shape[2]))\n",
        "\n",
        "trainxs_flat = flatten_2D_images(trainxs)\n",
        "devxs_flat = flatten_2D_images(devxs)\n",
        "testxs_flat = flatten_2D_images(testxs)\n",
        "\n",
        "print(f\"Original train data shape: {trainxs.shape}\")\n",
        "print(f\"Flattened train data shape: {trainxs_flat.shape}\")\n",
        "print(f\"Original dev data shape: {devxs.shape}\")\n",
        "print(f\"Flattened dev data shape: {devxs_flat.shape}\")\n",
        "print(f\"Original test data shape: {testxs.shape}\")\n",
        "print(f\"Flattened test data shape: {testxs_flat.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baB3ZY_Pc6ya"
      },
      "source": [
        "##Useful Values##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vO8sCsQMc-YL"
      },
      "source": [
        "# Number of examples in each dataset\n",
        "train_n = trainxs_flat.shape[1]\n",
        "dev_n = devxs_flat.shape[1]\n",
        "test_n = testxs_flat.shape[1]\n",
        "\n",
        "# Number of dimesions in each image\n",
        "dims = trainxs_flat.shape[0]\n",
        "\n",
        "# Pixel value range\n",
        "pixel_value_max = np.amax(trainxs_flat)\n",
        "pixel_value_min = np.amin(trainxs_flat)\n",
        "print(f\"Max pixel value: {pixel_value_max}\")\n",
        "print(f\"Min pixel value: {pixel_value_min}\")\n",
        "\n",
        "NUMBER_OF_EPOCHS = 500"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebboR5ba7gJ_"
      },
      "source": [
        "##Common Functions##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFfqsg8Cddc9"
      },
      "source": [
        "# Copy, paste and modify this code for accuracy and loss plots\n",
        "# The input for each is two lists one for training dataset, and one for the dev dataset\n",
        "\n",
        "# Accuracy\n",
        "list_of_train_accuracies = [] # change the name of variable so don't cause clashes\n",
        "list_of_dev_accuracies = [] # change the name of variable so don't cause clashes\n",
        "\n",
        "plt.plot(list_of_train_accuracies, color='k', linestyle='-')\n",
        "plt.plot(list_of_dev_accuracies, color='r', linestyle='-')\n",
        "plt.title(f'Model accuracy',  color='k')\n",
        "plt.ylabel('Accuracy',  color='k')\n",
        "plt.xlabel('Epoch',  color='k')\n",
        "plt.legend(['Training', 'Validation'], loc='upper left')\n",
        "plt.tick_params(colors='k')\n",
        "plt.xlim(0, NUMBER_OF_EPOCHS)\n",
        "plt.ylim(0, 1)\n",
        "plt.show()\n",
        "\n",
        "# Loss\n",
        "list_of_train_loss = []\n",
        "list_of_dev_loss = []\n",
        "\n",
        "plt.plot(list_of_train_loss, color='k', linestyle='-')\n",
        "plt.plot(list_of_dev_loss, color='r', linestyle='-')\n",
        "plt.title(f'Model loss',  color='k')\n",
        "plt.ylabel('loss',  color='k')\n",
        "plt.xlabel('Epoch',  color='k')\n",
        "plt.legend(['Training', 'Validation'], loc='upper left')\n",
        "plt.tick_params(colors='k')\n",
        "plt.xlim(0, NUMBER_OF_EPOCHS)\n",
        "plt.ylim(0, 1)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBjfttyP7EH9"
      },
      "source": [
        "## Assignment 1.2: Mean squared-loss logistic regression ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsAz6CG57F14"
      },
      "source": [
        "# 2.2\n",
        "def sigmoid(z):\n",
        "  return 1/(1 + np.exp(z))\n",
        "\n",
        "def f(w, b, x):\n",
        "  return sigmoid(np.dot(w, x) + b)\n",
        "\n",
        "def loss(w, b, x, y):\n",
        "  \"\"\"Compute the the mean-squared loss\"\"\"\n",
        "  return -(1/2)*(y-f(w, b, x))**2\n",
        "\n",
        "def df_dw_fd(w, b, x, y, epsilon = 10**(-5)):\n",
        "  \"\"\"Compute the gradient with respect to the weights using finite differences\"\"\"\n",
        "  gradients = []\n",
        "  for i in range(0, w.shape[0]):\n",
        "    w_i = w[i]\n",
        "    w[i] += epsilon / 2.0\n",
        "    lhs = loss(w, b, x, y)\n",
        "    w[i] = w_i\n",
        "    w[i] -= epsilon / 2.0\n",
        "    rhs = loss(w, b, x, y)\n",
        "    w[i] = w_i\n",
        "    df_dw_i = (lhs - rhs) / epsilon\n",
        "    gradients.append(df_dw_i)\n",
        "  return gradients\n",
        "\n",
        "def df_db_fd(w, b, x, y, epsilon = 10**(-5)):\n",
        "  \"\"\"Compute the gradient with respect to the bias using finite differences\"\"\"\n",
        "  lhs = loss(w, b + epsilon / 2.0, x, y)\n",
        "  rhs = loss(w, b - epsilon / 2.0, x, y)\n",
        "  return (lhs - rhs) / epsilon\n",
        "\n",
        "def df_dw_analytical(w, b, x, y):\n",
        "  \"\"\"Compute the gradient with respect to the weights analytically\"\"\"\n",
        "  y_hat = f(w, b, x)\n",
        "  return -(y - y_hat)*y_hat*(1-y_hat)*x\n",
        "\n",
        "def df_db_analytical(w, b, x, y):\n",
        "  \"\"\"Compute the gradient with respect to the weights analytically\"\"\"\n",
        "  y_hat = f(w, b, x)\n",
        "  return -(y - y_hat)*y_hat*(1-y_hat)\n",
        "\n",
        "training_samples = list(range(38,41))\n",
        "\n",
        "for i in training_samples:\n",
        "  x, y = trainxs_flat[:,i], trainys[i]\n",
        "\n",
        "  w = np.random.normal(0, 1, dims)/100\n",
        "  b = 0.0\n",
        "\n",
        "  fd_bias = df_db_fd(w, b, x, y)\n",
        "  fd_weights = df_dw_fd(w, b, x, y)\n",
        "\n",
        "  analytical_bias = df_db_analytical(w, b, x, y)\n",
        "  analytical_weights = df_dw_analytical(w, b, x, y)\n",
        "\n",
        "  absolute_bias_difference = np.absolute(fd_bias - analytical_bias)\n",
        "  absolute_weight_differences = np.absolute(fd_weights - analytical_weights)\n",
        "\n",
        "  print(f\"Training set index {i}, y is {y}, y_hat is {f(w, b, x)}\")\n",
        "  print('Finite differences estimates:')\n",
        "  print(f\"* Bias: {fd_bias}\")\n",
        "  print(f\"* Weights: {fd_weights}\")\n",
        "  print('Analytical gradient estimates:')\n",
        "  print(f\"* Bias: {analytical_bias}\")\n",
        "  print(f\"* Weights: {analytical_weights.tolist()}\")\n",
        "  print('Differences')\n",
        "  print(f\"* Bias difference: {absolute_bias_difference}\")\n",
        "  print(f\"* Bias difference less than e^(-10)? {absolute_bias_difference < np.e**(-10)}\")\n",
        "  print(f\"* Sample weight differences: {np.extract(x > 0, absolute_weight_differences)[0:10].tolist()}\")\n",
        "  print(f\"* Weight differences all less than e^(-10)? {(absolute_weight_differences < np.e**(-10)).all()}\")\n",
        "  print('\\n')\n",
        "\n",
        "# 2.3 - 2.4\n",
        "def predict(w, b, x):\n",
        "  \"\"\"Predict the class of a feature vector x given some weights and a bias\"\"\"\n",
        "  return 1 if f(w, b, x) >= 0.5 else 0\n",
        "\n",
        "def accuracy(xs, ys, w, b):\n",
        "  \"\"\"Compute the accuracy for a set of feature vectors and their targets given\n",
        "  some weights and a bias\"\"\"\n",
        "  n = xs.shape[1]\n",
        "\n",
        "  correct = 0\n",
        "  for i in range(n):\n",
        "    x = xs[:,i]\n",
        "    y = ys[i]\n",
        "    if predict(w, b, x) == y:\n",
        "      correct += 1\n",
        "  return correct/n\n",
        "\n",
        "train_loss_per_epoch = []\n",
        "train_accuracy_per_epoch = []\n",
        "dev_accuracy_per_epoch = []\n",
        "\n",
        "def train_logistic_regression(training_xs, training_ys, max_epochs=50, learning_rate=0.01):\n",
        "  \"\"\"Train a logistic regression model for a set of feature vectors and their\n",
        "  targets, for a given learning rate, and a maximum number of epochs unless\n",
        "  it meets the convergence criterion before this epoch\"\"\"\n",
        "  assert(training_xs.shape[1] == training_ys.shape[0])\n",
        "\n",
        "  train_n = training_xs.shape[1]\n",
        "  dims = training_xs.shape[0]\n",
        "  w = np.random.normal(0, 1, dims)/100\n",
        "  b = 0.0\n",
        "\n",
        "  for epoch in range(max_epochs):\n",
        "    grad_w = np.zeros(dims)\n",
        "    grad_b = 0\n",
        "\n",
        "    mean_squared_loss = 0\n",
        "\n",
        "    shuffled_indices = [*range(0,train_n,1)]\n",
        "    np.random.shuffle(shuffled_indices)\n",
        "\n",
        "    for i in shuffled_indices:\n",
        "      x, y = training_xs[:,i], training_ys[i]\n",
        "      y_hat = f(w, b, x)\n",
        "\n",
        "      grad_w -= (y - y_hat) * y_hat * (1 - y_hat) * x\n",
        "      grad_b -= (y - y_hat) * y_hat * (1 - y_hat)\n",
        "\n",
        "      mean_squared_loss += (y - y_hat)**2/(2*train_n)\n",
        "\n",
        "    grad_w /= train_n\n",
        "    grad_b /= train_n\n",
        "\n",
        "    w += learning_rate * grad_w\n",
        "    b += learning_rate * grad_b\n",
        "\n",
        "    train_accuracy = accuracy(training_xs, training_ys, w, b)\n",
        "    dev_accuracy = accuracy(devxs_flat, devys, w, b)\n",
        "\n",
        "    train_loss_per_epoch.append(mean_squared_loss)\n",
        "    train_accuracy_per_epoch.append(train_accuracy)\n",
        "    dev_accuracy_per_epoch.append(dev_accuracy)\n",
        "\n",
        "    # Early stopping (convergence criterion)\n",
        "    if len(train_accuracy_per_epoch) >= 30:\n",
        "      if np.mean(train_accuracy_per_epoch[-15:]) - np.mean(train_accuracy_per_epoch[-30:-15]) < 0.001:\n",
        "          print(f\"Break on epoch {epoch}\")\n",
        "          break\n",
        "\n",
        "  return w, b\n",
        "\n",
        "MAX_EPOCHS = 1000\n",
        "LEARNING_RATE = 0.01\n",
        "w_hat, b_hat = train_logistic_regression(trainxs_flat, trainys, learning_rate=LEARNING_RATE, max_epochs=MAX_EPOCHS)\n",
        "\n",
        "# 2.5 — Loss\n",
        "\n",
        "plt.plot(train_loss_per_epoch, color='k', linestyle='-')\n",
        "plt.title(f'Logistic regression: Loss (learning rate = {LEARNING_RATE})',  color='k')\n",
        "plt.ylabel('Loss',  color='k')\n",
        "plt.xlabel('Epoch',  color='k')\n",
        "plt.legend(['Training', 'Validation'], loc='upper left')\n",
        "plt.tick_params(colors='k')\n",
        "plt.xlim(0, len(train_loss_per_epoch))\n",
        "plt.ylim(0, 1)\n",
        "plt.savefig(f\"q2_loss_{LEARNING_RATE}.png\", dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# 2.6 — Accuracy\n",
        "\n",
        "plt.plot(train_accuracy_per_epoch, color='k', linestyle='-')\n",
        "plt.plot(dev_accuracy_per_epoch, color='r', linestyle='-')\n",
        "plt.title(f'Logistic regression: Accuracy (learning rate = {LEARNING_RATE})',  color='k')\n",
        "plt.ylabel('Accuracy',  color='k')\n",
        "plt.xlabel('Epoch',  color='k')\n",
        "plt.legend(['Training', 'Validation'], loc='upper left')\n",
        "plt.tick_params(colors='k')\n",
        "plt.xlim(0, len(train_accuracy_per_epoch))\n",
        "plt.ylim(0, 1)\n",
        "plt.savefig(f\"q2_accuracy_{LEARNING_RATE}.png\", dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# 2.7 — Epoch with highest validation accuracy\n",
        "index_of_maximum = np.argmax(np.array(dev_accuracy_per_epoch))\n",
        "epoch = index_of_maximum + 1\n",
        "print(f\"Epoch: {epoch}\")\n",
        "print(f\"Training accuracy on epoch {epoch}: {train_accuracy_per_epoch[index_of_maximum]}\")\n",
        "print(f\"Validation accuracy on epoch {epoch}: {dev_accuracy_per_epoch[index_of_maximum]}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}