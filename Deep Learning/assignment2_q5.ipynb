{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Q5",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbojmgpQzCEa"
      },
      "source": [
        "import collections\n",
        "import gensim\n",
        "import gzip\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import re\n",
        "import string\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import ParameterGrid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4ITTs086BJz"
      },
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "path = \"/content/drive/My Drive/UCL/Modules/DL/assignment2/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjM1wnOh4y1n"
      },
      "source": [
        "# Load Word2Vec Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esVNg1NHfiZV"
      },
      "source": [
        "!wget https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
        "\n",
        "# Load Google's pre-trained Word2Vec model\n",
        "w2v = gensim.models.KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin.gz\", binary=True, limit=1000000)\n",
        "\n",
        "# Normalise the vectors\n",
        "w2v.init_sims(replace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQvfqGg-4_up"
      },
      "source": [
        "token2index_map = {\n",
        "    word: idx for idx, word in enumerate(w2v.vocab)\n",
        "}\n",
        "\n",
        "def token2index(token):\n",
        "  \"\"\"\n",
        "  Get the word2vec embedding index for a token\n",
        "  Input:\n",
        "  - Token = a string\n",
        "  Output:\n",
        "  - The index of the word2vec embedding of the token\"\"\"\n",
        "  # Use \"UNK\" as missing token embedding\n",
        "  default = token2index_map.get(\"UNK\")\n",
        "\n",
        "  # Remove multi-word tokens to match embeddings in Julia starter code\n",
        "  if \"_\" in token:\n",
        "    return default\n",
        "\n",
        "  return token2index_map.get(token, default)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md_eSMhl5DR5"
      },
      "source": [
        "# Load Stanford sentiment treebank dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVfwmvA25HEb"
      },
      "source": [
        "# Load stanford sentiment treebank dataset\n",
        "# Site for more information on data set: https://nlp.stanford.edu/sentiment/\n",
        "\n",
        "!wget https://nlp.stanford.edu/sentiment/trainDevTestTrees_PTB.zip\n",
        "!unzip trainDevTestTrees_PTB.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNFKdtm35Mbp"
      },
      "source": [
        "# Parse the tree into sentences and classifications\n",
        "\n",
        "def parse_tree(sentence_list_tree):\n",
        "    \"\"\"\n",
        "    Function for extracting text only from sentiment tree (remove tree structure)\n",
        "    Input: list of strings of trees of sentences/lines\n",
        "    Output: x_list is a list of sentences, y_list is a list of sentence sentiment classes\n",
        "    \"\"\"\n",
        "    # remove empty strings from list\n",
        "    while(\"\" in sentence_list_tree): \n",
        "        sentence_list_tree.remove(\"\")\n",
        "    \n",
        "    # initialise lists to store x (the sentence), y (the sentiment)\n",
        "    x_list = []\n",
        "    y_list = []\n",
        "    for sentence in sentence_list_tree:\n",
        "        y_list.append(int(sentence[1])) # y = sentiment class of sentence\n",
        "        # remove digits and parentheses using regular expression\n",
        "        patterns = r'[0-9]|\\(|\\)' \n",
        "        sentence = re.sub(patterns, '', sentence)\n",
        "        # remove -RRB- and -LRB-\n",
        "        sentence = re.sub(r\"-RRB-\", \"\", sentence)\n",
        "        sentence = re.sub(r\"-LRB-\", \"\", sentence)\n",
        "        # remove duplicate spaces\n",
        "        sentence = \" \".join(sentence.split())\n",
        "        # remove spaces before some types of punctuation\n",
        "        sentence = re.sub(r\"\\s([?.!;\\,](?:\\s|$))\", r'\\1', sentence)\n",
        "        # remove spaces before apostrophes\n",
        "        sentence = re.sub(r\" '\\b\", \"'\", sentence)\n",
        "        # remove spaces before n't\n",
        "        sentence = re.sub(r\" n't\\b\", \"n't\", sentence)\n",
        "        # remove space before ...\n",
        "        sentence = re.sub(r\" \\.\\.\\.\", r\"...\", sentence)\n",
        "        # make all lower case\n",
        "        sentence = sentence.lower()\n",
        "        # remove slashes (replace with space)\n",
        "        sentence = sentence.replace(\"\\\\\",\" \")\n",
        "\n",
        "        # remove dashes (replace with space)\n",
        "        sentence = re.sub(\"-\", \" \", sentence)\n",
        "        # remove all other punctuation\n",
        "        sentence = re.sub('[!\"#$%&()*+,/:;<=>?@[\\]^_`{|}~]', \"\", sentence)\n",
        "        # remove double quotation marks\n",
        "        sentence = re.sub(\"''\", \"\", sentence)\n",
        "        # remove single quotation marks (space beforehand so as to not replace apostrophes)\n",
        "        sentence = re.sub(\" '\", \" \", sentence)\n",
        "        # remove duplicate spaces\n",
        "        sentence = \" \".join(sentence.split())\n",
        "\n",
        "        # replace letters with accents, with standard non-accented english letters\n",
        "        sentence = re.sub('[àáâã]', \"a\", sentence)\n",
        "        sentence = re.sub('[æ]', \"ae\", sentence)\n",
        "        sentence = re.sub('[ç]', \"c\", sentence)\n",
        "        sentence = re.sub('[èé]', \"e\", sentence)\n",
        "        sentence = re.sub('[íï]', \"i\", sentence)\n",
        "        sentence = re.sub('[ñ]', \"n\", sentence)\n",
        "        sentence = re.sub('[óôö]', \"o\", sentence)\n",
        "        sentence = re.sub('[ûü]', \"u\", sentence)\n",
        "\n",
        "        x_list.append(sentence)\n",
        "\n",
        "    return x_list, y_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFxv5uC55PEY"
      },
      "source": [
        "# Load and parse string data\n",
        "\n",
        "train_file = open(\"/content/trees/train.txt\", \"r\")\n",
        "train_tree = train_file.read()\n",
        "train_tree_sentence_list = train_tree.split(\"\\n\")\n",
        "x_train_list, y_train_list = parse_tree(train_tree_sentence_list)\n",
        "train_length = len(x_train_list)\n",
        "\n",
        "dev_file = open(\"/content/trees/dev.txt\",\"r\")\n",
        "dev_tree = dev_file.read()\n",
        "dev_tree_sentence_list = dev_tree.split(\"\\n\")\n",
        "x_dev_list, y_dev_list = parse_tree(dev_tree_sentence_list)\n",
        "dev_length = len(x_dev_list)\n",
        "\n",
        "test_file = open(\"/content/trees/test.txt\", \"r\")\n",
        "test_tree = test_file.read()\n",
        "test_tree_sentence_list = test_tree.split(\"\\n\")\n",
        "x_test_list, y_test_list = parse_tree(test_tree_sentence_list)\n",
        "test_length = len(x_test_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7lO2PcocVsz"
      },
      "source": [
        "# Data processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcE6Fnq2xM2f"
      },
      "source": [
        "class SentenceBatcher:\n",
        "    def __init__(self, inputs, labels, batch_size=128, drop_last=False):\n",
        "        # Tokenise the input sentences\n",
        "        tokenised_inputs = np.array([sentence.split() for sentence in inputs])\n",
        "\n",
        "        # Map the tokens to word2vec indices\n",
        "        indices_mapping = [torch.tensor([token2index(token) for token in tokens]) for tokens in tokenised_inputs]\n",
        "\n",
        "        # Store sentences by length\n",
        "        self.sentences_by_length = {}\n",
        "        for input, label in zip(indices_mapping, labels):\n",
        "            length = input.shape[0]\n",
        "            \n",
        "            if length not in self.sentences_by_length:\n",
        "                self.sentences_by_length[length] = []\n",
        "            self.sentences_by_length[length].append([input, label])\n",
        "         \n",
        "        #  Create a DataLoader for each set of sentences of the same length\n",
        "        self.loaders = {length : torch.utils.data.DataLoader(\n",
        "                                  sentences,\n",
        "                                  batch_size=batch_size,\n",
        "                                  shuffle=True,\n",
        "                                  drop_last=drop_last)\n",
        "          for length, sentences in self.sentences_by_length.items()}\n",
        "        \n",
        "    def __iter__(self):\n",
        "        # Create an iterator for each sentence length\n",
        "        iters = [iter(loader) for loader in self.loaders.values()]\n",
        "        while iters:\n",
        "            # Get a random iterator\n",
        "            i = random.choice(iters)\n",
        "            try:\n",
        "                yield next(i)\n",
        "            except StopIteration:\n",
        "                iters.remove(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpX-V7kbkiLB"
      },
      "source": [
        "# Data exploration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQgUjNSrkk0g"
      },
      "source": [
        "# Display example data for training set\n",
        "\n",
        "start_index = 0\n",
        "end_index = 30\n",
        "\n",
        "for sentiment, sentence in zip(y_train_list[start_index:end_index], x_train_list[start_index:end_index]):\n",
        "  print(f\"y = {sentiment} | x = ({sentence})\")\n",
        "\n",
        "# Number of examples in each dataset\n",
        "\n",
        "print(f\"Number of sentences training set: {len(x_train_list)}\")\n",
        "print(f\"Number of sentences validation set: {len(x_dev_list)}\")\n",
        "print(f\"Number of sentences test set: {len(x_test_list)}\")\n",
        "\n",
        "# Plot distribution of sentiment classifications by dataset\n",
        "from collections import Counter\n",
        "\n",
        "train_counter = Counter(y_train_list)\n",
        "plt.bar(train_counter.keys(), train_counter.values())\n",
        "plt.title(\"Number of examples per classification\")\n",
        "plt.ylabel('Count')\n",
        "plt.xlabel('Classification')\n",
        "plt.savefig(f\"sentence_lengths.png\", dpi=300)\n",
        "plt.show()\n",
        "\n",
        "dev_counter = Counter(y_dev_list)\n",
        "plt.bar(dev_counter.keys(), dev_counter.values())\n",
        "plt.title(\"Number of examples per classification\")\n",
        "plt.ylabel('Count')\n",
        "plt.xlabel('Classification')\n",
        "plt.savefig(f\"sentence_lengths.png\", dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# Plot distribution of sentence lengths by dataset\n",
        "\n",
        "train_sentence_lengths = Counter(map(lambda s : len(s.split()), x_train_list))\n",
        "plt.bar(train_sentence_lengths.keys(), train_sentence_lengths.values())\n",
        "plt.title(\"Number of tokens per sentence\")\n",
        "plt.ylabel('Count')\n",
        "plt.xlabel('Number of tokens')\n",
        "plt.savefig(f\"sentence_lengths.png\", dpi=300)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7k_EnEaHQrBb"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVnONj86tUh8"
      },
      "source": [
        "class RNN(nn.Module):\n",
        "  def __init__(self, embedding_size, hidden_size, num_layers, num_classes, cell_type, num_dense_layers, hidden_to_dense_size_ratio=2, dropout=0, fixed_embeddings=True):\n",
        "    super(RNN, self).__init__()\n",
        "    self.num_layers = num_layers\n",
        "    self.hidden_size = hidden_size\n",
        "    self.cell_type = cell_type\n",
        "    self.num_dense_layers = num_dense_layers\n",
        "\n",
        "    self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(w2v.vectors))\n",
        "    self.embedding.weight.requires_grad = not fixed_embeddings\n",
        "\n",
        "    # Only one of these will be used in forward statement (as specified by hyperparam cell_type for current run)\n",
        "    self.rnn = nn.RNN(embedding_size, hidden_size, num_layers, dropout=dropout, batch_first = True)\n",
        "    self.gru = nn.GRU(embedding_size, hidden_size, num_layers, dropout=dropout, batch_first = True)\n",
        "    self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=dropout, batch_first = True)\n",
        "    \n",
        "    # Number of these linear layers that gets used depends on hyperparam num_dense_layers\n",
        "    if num_dense_layers == 1:\n",
        "      self.linear = nn.Linear(hidden_size, num_classes)\n",
        "    elif num_dense_layers == 2:\n",
        "      self.linear1 = nn.Linear(hidden_size, hidden_size//hidden_to_dense_size_ratio)\n",
        "      self.linear2 = nn.Linear(hidden_size//hidden_to_dense_size_ratio, num_classes)\n",
        "      self.dropout = nn.Dropout(dropout)\n",
        "  \n",
        "  def forward(self, x_batch):\n",
        "    # Get the word embeddings\n",
        "    embedded = self.embedding(x_batch)\n",
        "        \n",
        "    if self.cell_type == 'VanillaRNN':\n",
        "      packed_out, _ = self.rnn(embedded)\n",
        "    elif self.cell_type == 'GRU':\n",
        "      packed_out, _ = self.gru(embedded)\n",
        "    elif self.cell_type == 'LSTM':\n",
        "      packed_out, _ = self.lstm(embedded)\n",
        "\n",
        "    # Extract the final output and feed into a dense layer\n",
        "    out = packed_out[:, -1, :]\n",
        "    if self.num_dense_layers == 1:\n",
        "      out = self.linear(out)\n",
        "    elif self.num_dense_layers == 2:\n",
        "      out = self.linear1(out)\n",
        "      out = torch.relu(out)\n",
        "      out = self.dropout(out)\n",
        "      out = self.linear2(out)\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KULiX4GXPt9"
      },
      "source": [
        "# Model, loss, optimizer, and training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuHG0Kbf-AgH"
      },
      "source": [
        "# Hyperparameters to grid search over\n",
        "hyperparam_grid = {'cell_type': ['GRU'],\n",
        "                   'num_layers': [3],\n",
        "                   'num_dense_layers': [2],\n",
        "                   'hidden_size': [128],\n",
        "                   'hidden_to_dense_size_ratio': [2],\n",
        "                   'learning_rate': [0.001],\n",
        "                   'dropout': [0.25],\n",
        "                   'fixed_embeddings': [True],\n",
        "                   'batch_size': [128]\n",
        "}\n",
        "\n",
        "# Initialise dataframe to store results of grid search \n",
        "results_df = pd.DataFrame()\n",
        "\n",
        "# Range of epochs\n",
        "MIN_EPOCHS = 20\n",
        "MAX_EPOCHS = 100\n",
        "\n",
        "for run_index, hyperparams in enumerate(ParameterGrid(hyperparam_grid)):\n",
        "\n",
        "  # Get training and validation data loaders\n",
        "  train_loader = SentenceBatcher(x_train_list, y_train_list, hyperparams['batch_size'], drop_last=False)\n",
        "  dev_loader = SentenceBatcher(x_dev_list, y_dev_list, hyperparams['batch_size'], drop_last=False)\n",
        "\n",
        "  # Initialise model\n",
        "  model = RNN(embedding_size=300,\n",
        "              hidden_size=hyperparams['hidden_size'],\n",
        "              num_layers=hyperparams['num_layers'],\n",
        "              num_classes=5,\n",
        "              cell_type=hyperparams['cell_type'],\n",
        "              num_dense_layers=hyperparams['num_dense_layers'],\n",
        "              hidden_to_dense_size_ratio=hyperparams['hidden_to_dense_size_ratio'],\n",
        "              dropout=hyperparams['dropout'],\n",
        "              fixed_embeddings=hyperparams['fixed_embeddings'])\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  model.to(device)\n",
        "\n",
        "  # Loss & optimizer\n",
        "  criterion = nn.CrossEntropyLoss() # note that this will apply the softmax for us\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=hyperparams['learning_rate'])\n",
        "\n",
        "  # Training loop\n",
        "  running_loss = 0\n",
        "  running_corrects = 0\n",
        "  training_accuracy_list = []\n",
        "  training_loss_list = []\n",
        "\n",
        "  best_epoch = 0\n",
        "  best_train_accuracy = 0\n",
        "  best_dev_accuracy = 0\n",
        "\n",
        "  dev_accuracy_list = []\n",
        "  dev_loss_list = []\n",
        "\n",
        "  converged = False\n",
        "  epoch = 0\n",
        "  epochs_to_plot = []\n",
        "\n",
        "  while (converged == False) and (epoch <= MAX_EPOCHS):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    running_loss = 0\n",
        "    running_corrects = 0\n",
        "    num_of_batches = 0\n",
        "\n",
        "    # Generate batch\n",
        "    for inputs, labels in train_loader: \n",
        "      model.train()\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # Send to GPU\n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      # Forward pass\n",
        "      predicted = model(inputs)\n",
        "\n",
        "      # Loss\n",
        "      loss = criterion(predicted, labels)\n",
        "      running_loss += loss.item()\n",
        "\n",
        "      # Backward pass\n",
        "      loss.backward()\n",
        "\n",
        "      # Updates\n",
        "      optimizer.step()\n",
        "\n",
        "      num_of_batches +=1\n",
        "\n",
        "      # Get number of correct training predictions per batch and sum\n",
        "      _, train_pred = torch.max(predicted, 1)\n",
        "      running_corrects += (train_pred == labels).sum().item()\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      # Calculate training accuracy per epoch\n",
        "      training_accuracy = running_corrects/train_length\n",
        "      training_accuracy_list.append(training_accuracy)\n",
        "\n",
        "      training_loss_list.append(running_loss/num_of_batches)\n",
        "\n",
        "      # Evaluate on validation dataset\n",
        "\n",
        "      dev_running_loss = 0\n",
        "      dev_running_corrects = 0\n",
        "      num_of_batches = 0\n",
        "\n",
        "      for dev_inputs, dev_labels in dev_loader: \n",
        "\n",
        "          # Send to GPU\n",
        "          dev_inputs = dev_inputs.to(device)\n",
        "          dev_labels = dev_labels.to(device)\n",
        "\n",
        "          # Forward pass\n",
        "          dev_predicted = model(dev_inputs)\n",
        "\n",
        "          # Loss\n",
        "          dev_loss = criterion(dev_predicted, dev_labels)\n",
        "          dev_running_loss += dev_loss.item()\n",
        "\n",
        "          num_of_batches +=1\n",
        "\n",
        "          # Get number of correct validation predictions per batch and sum\n",
        "          _, dev_pred = torch.max(dev_predicted, 1)\n",
        "          dev_running_corrects += (dev_pred == dev_labels).sum().item()\n",
        "          \n",
        "      dev_accuracy = dev_running_corrects/dev_length\n",
        "      dev_accuracy_list.append(dev_accuracy)\n",
        "\n",
        "      dev_loss_list.append(dev_running_loss/num_of_batches)\n",
        "\n",
        "      print(f\"Epoch: {epoch}, train_loss: {loss.item():.4f}, train_acc: {training_accuracy:.4f}, val_loss: {dev_loss.item():.4f}, val_acc: {dev_accuracy:.4f}\")\n",
        "\n",
        "      epochs_to_plot.append(epoch)\n",
        "\n",
        "      # Store the current results if it is the highest validation accuracy so far\n",
        "      if dev_accuracy > best_dev_accuracy:\n",
        "          best_dev_accuracy = dev_accuracy  \n",
        "          best_dev_loss = dev_loss.item()\n",
        "          best_train_accuracy = training_accuracy\n",
        "          best_loss = loss.item()\n",
        "          best_epoch = epoch\n",
        "          best_model = model\n",
        "        \n",
        "      # Early stopping criteria\n",
        "      if epoch >= MIN_EPOCHS:\n",
        "        if (np.mean(dev_loss_list[-15:]) - np.mean(dev_loss_list[-30:-15])) > 0:\n",
        "          converged = True\n",
        "          print(\"Model Converged\")\n",
        "\n",
        "    epoch += 1\n",
        "\n",
        "  hyperparams['model_index'] = run_index + 1\n",
        "  hyperparams['best_epoch'] = best_epoch\n",
        "  hyperparams['best_validation_accuracy'] = best_dev_accuracy\n",
        "  hyperparams['associated_validation_loss'] = best_dev_loss\n",
        "  hyperparams['associated_training_accuracy'] = best_train_accuracy\n",
        "  hyperparams['associated_training_loss'] = best_loss\n",
        "\n",
        "  results_df = results_df.append(hyperparams , ignore_index=True)\n",
        "  results_df = results_df.sort_values(\"best_validation_accuracy\", ascending=False).round(4)\n",
        "  results_df.to_csv(f\"{path}RESULTS_TABLE_{hyperparams['cell_type']}.csv\")\n",
        "\n",
        "  print(f\"\\n RUN_INDEX: {run_index + 1} \\n\")\n",
        "  print(hyperparams)\n",
        "\n",
        "  # Save the model\n",
        "  torch.save(best_model, f\"{path}{hyperparams['cell_type']}_{best_dev_accuracy:.4f}_devacc_{best_train_accuracy:.4f}_trainacc.pth\")\n",
        "\n",
        "  # Loss plot\n",
        "  plt.plot(epochs_to_plot, training_loss_list, color='k', linestyle='-')\n",
        "  plt.plot(epochs_to_plot, dev_loss_list, color='r', linestyle='-')\n",
        "  plt.legend(['Training', 'Validation'], loc='upper right')\n",
        "  plt.ylabel('Loss', color='k')\n",
        "  plt.xlabel('Epoch', color='k')\n",
        "  plt.title(f'''Loss plot ({hyperparams['cell_type']}, Val Accuracy {best_dev_accuracy:.4f}) \\n \n",
        "  Hidden size: {hyperparams['hidden_size']}, Recurrent layers: {hyperparams['num_layers']}, Dense layers: {hyperparams['num_dense_layers']}, \n",
        "  Hidden to dense size ratio: {hyperparams['hidden_to_dense_size_ratio']}, LR: {hyperparams['learning_rate']}, Dropout: {hyperparams['dropout']}, \n",
        "  Fixed embeddings: {hyperparams['fixed_embeddings']}, Batch size: {hyperparams['batch_size']}''', color='k')\n",
        "\n",
        "  plt.savefig(f\"{path}{hyperparams['cell_type']}_{best_dev_accuracy:.4f}_devacc_{best_train_accuracy:.4f}_trainacc_LOSSPLOT.png\", dpi=300, bbox_inches = \"tight\")\n",
        "  plt.show()\n",
        "\n",
        "  # Accuracy plot\n",
        "  plt.plot(epochs_to_plot, training_accuracy_list, color='k', linestyle='-')\n",
        "  plt.plot(epochs_to_plot, dev_accuracy_list, color='r', linestyle='-')\n",
        "  plt.legend(['Training', 'Validation'], loc='lower right')\n",
        "  plt.ylabel('Accuracy', color='k')\n",
        "  plt.xlabel('Epoch', color='k')\n",
        "  plt.title(f'''Accuracy plot ({hyperparams['cell_type']}, Val Accuracy {best_dev_accuracy:.4f}) \\n \n",
        "  Hidden size: {hyperparams['hidden_size']}, Recurrent layers: {hyperparams['num_layers']}, Dense layers: {hyperparams['num_dense_layers']},  \n",
        "  Hidden to dense size ratio: {hyperparams['hidden_to_dense_size_ratio']}, LR: {hyperparams['learning_rate']}, Dropout: {hyperparams['dropout']},\n",
        "  Fixed embeddings: {hyperparams['fixed_embeddings']}, Batch size: {hyperparams['batch_size']}''', color='k')\n",
        "\n",
        "  plt.savefig(f\"{path}{hyperparams['cell_type']}_{best_dev_accuracy:.4f}_devacc_{best_train_accuracy:.4f}_trainacc_ACCPLOT.png\", dpi=300, bbox_inches = \"tight\")\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1W4CxR96oVlf"
      },
      "source": [
        "# Evaluate on test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62dhyRzPyDZ7"
      },
      "source": [
        "# Load best model (highest validation accuracy) for further evaluation\n",
        "\n",
        "best_model = torch.load(f\"{path}GRU_0.4505_devacc_0.4896_trainacc.pth\")\n",
        "best_model.eval()\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "best_model.to(device)\n",
        "\n",
        "# Get test data loader\n",
        "test_loader = SentenceBatcher(x_test_list, y_test_list, hyperparams['batch_size'], drop_last=False)\n",
        "\n",
        "# Loss & optimiser\n",
        "criterion = nn.CrossEntropyLoss() # note that this will apply the softmax for us\n",
        "\n",
        "# Final performance of best model on test set \n",
        "\n",
        "test_running_loss = 0\n",
        "test_running_corrects = 0\n",
        "\n",
        "for test_inputs, test_labels in test_loader: \n",
        "\n",
        "    # Send to GPU\n",
        "    test_inputs = test_inputs.to(device)\n",
        "    test_labels = test_labels.to(device)\n",
        "\n",
        "    # Forward pass\n",
        "    test_predicted = best_model(test_inputs)\n",
        "\n",
        "    # Test loss\n",
        "    loss_test = criterion(test_predicted, test_labels)\n",
        "    test_running_loss += loss_test\n",
        "\n",
        "    # Test accuracy\n",
        "    _, test_pred = torch.max(test_predicted, 1)\n",
        "    test_running_corrects += (test_pred == test_labels).sum().item()\n",
        "\n",
        "test_loss_avg = test_running_loss/test_length\n",
        "test_accuracy_avg = test_running_corrects/test_length\n",
        "\n",
        "# print results\n",
        "print(f\"Final test accuracy: {test_accuracy_avg}\")\n",
        "print(f\"Final test loss: {test_loss_avg}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpSdo6m0jioR"
      },
      "source": [
        "# Online review classifications"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfMS-LJQjshM"
      },
      "source": [
        "# Online reviews, pre-processed as in parse_tree\n",
        "x_online_list = [\n",
        "                 \"to this day this is still my favorite pixar film the animation is stellar its heartwarming funny and proves that pixar movies are always bound to be great except for cars but thats a different story this has a shot at the title best movie of the century\", # https://www.imdb.com/review/rw5485122\n",
        "                 \"this is just a wonderful telling of charles dickens great christmas story the story being so good you would have to try had to make a bad movie out of it\", # https://www.imdb.com/review/rw0310420\n",
        "                 \"honestly i really should be giving this film a lower score somehow i enjoyed it quite a bit even in the face of the many fundamental issues which is a testament to the strength of the best sequences\", # https://www.imdb.com/review/rw4075393\n",
        "                 \"but the worst thing of all with this film is the mangling of austen's dialogue and the atrocious modern dialogue austen's dialogue needs no assistance from a writer who thinks he she can write like austen\", # https://www.imdb.com/review/rw1213354\n",
        "                 \"hours of boredom half the audience fell asleep including most of the kiddies beautiful to look at but that does not make for a interesting film\" # https://www.imdb.com/review/rw0717356\n",
        "]\n",
        "# SentenceBatcher will randomly order the reviews, so we keep track of their indices\n",
        "x_online_index = range(len(x_online_list))\n",
        "\n",
        "# Get online data loader\n",
        "online_loader = SentenceBatcher(x_online_list, x_online_index, batch_size=128, drop_last=False)\n",
        "\n",
        "# Evaluate model on online reviews\n",
        "\n",
        "for online_inputs, online_index in online_loader:\n",
        "  online_inputs = online_inputs.to(device)\n",
        "\n",
        "  online_predicted = best_model(online_inputs)\n",
        "  _, online_pred = torch.max(online_predicted, 1)\n",
        "\n",
        "  for index, pred in zip(online_index, online_pred):\n",
        "    print(f\"Review: {x_online_list[index]}\")\n",
        "    print(f\"Prediction: {pred}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}