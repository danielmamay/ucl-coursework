{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wC8kUorb4bfq"
   },
   "source": [
    "# Part I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EeANvCitk1LF"
   },
   "outputs": [],
   "source": [
    "# Install libraries for local development\n",
    "# !conda install --yes cvxopt numpy matplotlib pandas scipy\n",
    "\n",
    "# Import libraries\n",
    "\n",
    "from cvxopt import matrix, solvers\n",
    "from datetime import datetime\n",
    "from google.colab import drive\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from random import randint\n",
    "from scipy.spatial.distance import cdist\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_L_iXN_-zLDB"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EUADIc0_lMZr"
   },
   "outputs": [],
   "source": [
    "# Data preparation\n",
    "\n",
    "def load_data(path):\n",
    "  data = np.loadtxt(path)\n",
    "  xs = data[:,1:]\n",
    "  ys = np.ndarray.flatten(data[:,:1]).astype(int)\n",
    "  return xs, ys\n",
    "\n",
    "# Path for local development\n",
    "# path = \"/users/danielmay/Coursework/COMP0086/Coursework 2\"\n",
    "\n",
    "path = \"/content/drive/My Drive/Colab Notebooks/sl_coursework2\"\n",
    "\n",
    "data_xs, data_ys = load_data(f\"{path}/data/zipcombo.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-KggDElClYZ_"
   },
   "outputs": [],
   "source": [
    "# Data exploration\n",
    "\n",
    "# Display examples\n",
    "examples_to_show = list(range(17,18))\n",
    "\n",
    "for example in examples_to_show:\n",
    "  image = np.reshape(data_xs[example], (16, 16))\n",
    "\n",
    "  plt.imshow(image, cmap='gray')\n",
    "  plt.axis('off')\n",
    "  plt.title(f\"Label: {data_ys[example]}\\n\")\n",
    "  plt.savefig(f\"example_{example}\", dpi=300)\n",
    "  plt.show()\n",
    "\n",
    "# Display an example of each class\n",
    "fig, ax = plt.subplots(2,5)\n",
    "ax = ax.flatten()\n",
    "for i in range(10):\n",
    "    example = np.argwhere(data_ys == i)[0]\n",
    "    image = np.reshape(data_xs[example], (16, 16))\n",
    "    ax[i].imshow(image, cmap='gray')\n",
    "    ax[i].axis('off')\n",
    "fig.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig('all_classes', dpi=300)\n",
    "\n",
    "# Plot class counts\n",
    "counts = np.bincount(data_ys)\n",
    "print(\"Counts:\", counts)\n",
    "plt.bar(range(len(counts)), counts)\n",
    "plt.xticks(range(len(counts)))\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Number of examples')\n",
    "plt.title('Number of examples per class in the data set\\n')\n",
    "plt.savefig('examples_per_class', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rqh9bJj-7WYh"
   },
   "outputs": [],
   "source": [
    "# Data processing functions\n",
    "\n",
    "def split_data(xs, ys, frac=0.8):\n",
    "  \"\"\"\n",
    "  Randomly split data into a training set and test set\n",
    "  \"\"\"\n",
    "  n = xs.shape[0]\n",
    "  shuffled_indices = [*range(n)]\n",
    "  np.random.shuffle(shuffled_indices)\n",
    "  # Generate random training and test sets\n",
    "  training_n = int(frac * n)\n",
    "  training_indices, test_indices = shuffled_indices[:training_n], shuffled_indices[training_n:]\n",
    "  train_xs, test_xs = xs[training_indices], xs[test_indices]\n",
    "  train_ys, test_ys = ys[training_indices], ys[test_indices]\n",
    "\n",
    "  return train_xs, train_ys, test_xs, test_ys\n",
    "\n",
    "def generate_validation_folds(xs, ys, k):\n",
    "  \"\"\"\n",
    "  Randomly split data into k folds, returning a list of the form:\n",
    "  [train_xs, train_ys, validation_xs, validation_ys]\n",
    "  \"\"\"\n",
    "  result = []\n",
    "\n",
    "  n = xs.shape[0]\n",
    "  shuffled_indices = np.arange(n)\n",
    "  np.random.shuffle(shuffled_indices)\n",
    "  # Split the shuffled indices into k chunks for the validation sets\n",
    "  for validation_indices in np.array_split(shuffled_indices, k):\n",
    "      train_indices = np.setdiff1d(shuffled_indices, validation_indices)\n",
    "      # Extract training and validation sets\n",
    "      train_xs, validation_xs = xs[train_indices], xs[validation_indices]\n",
    "      train_ys, validation_ys = ys[train_indices], ys[validation_indices]\n",
    "      result.append([train_xs, train_ys, validation_xs, validation_ys])\n",
    "  return result\n",
    "\n",
    "def normalize_confusion_matrix(confusion_matrix):\n",
    "  \"\"\"\n",
    "  Normalize a confusion matrix of misclassifications\n",
    "  \"\"\"\n",
    "  return np.nan_to_num(confusion_matrix / confusion_matrix.sum(axis=1)[:,np.newaxis])\n",
    "\n",
    "# Kernel functions\n",
    "\n",
    "def polynomial_kernel(X, Y, p):\n",
    "  \"\"\"Compute polynomial kernel matrix, K, of x and y with exponent p\"\"\"\n",
    "  return np.dot(X, Y.T) ** p\n",
    "\n",
    "def gaussian_kernel(P, Q, c):\n",
    "  \"\"\"Compute Gaussian kernel matrix, K, of P and Q with width c\"\"\"\n",
    "  return np.exp(-c * cdist(P, Q) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "da2aDahKRghD"
   },
   "outputs": [],
   "source": [
    "MIN_EPOCHS = 10\n",
    "MAX_EPOCHS = 50\n",
    "\n",
    "class MultiClassPerceptronOvR:\n",
    "  \"\"\"\n",
    "  One-versus-rest (OvR) kernel perceptron\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, num_classes):\n",
    "    self.num_classes = num_classes\n",
    "    self.num_classifiers = num_classes\n",
    "\n",
    "    # Initialised by set_{polynomial/gaussian}_kernel method\n",
    "    self.kernel_function = None\n",
    "\n",
    "    # Initialised by train method\n",
    "    self.train_xs = None\n",
    "    self.alpha = None\n",
    "  \n",
    "  def set_polynomial_kernel(self, degree):\n",
    "    self.kernel_function = lambda x, y: polynomial_kernel(x, y, degree)\n",
    "  \n",
    "  def set_gaussian_kernel(self, width):\n",
    "    self.kernel_function = lambda p, q: gaussian_kernel(p, q, width)\n",
    "\n",
    "  def train(self, train_xs, train_ys):\n",
    "\n",
    "    # Initialisation\n",
    "    num_examples = train_xs.shape[0]\n",
    "    self.train_xs = train_xs\n",
    "    self.alpha = np.zeros((self.num_classifiers, num_examples))\n",
    "\n",
    "    # Compute Gram matrix, K\n",
    "    gram_matrix = self.kernel_function(train_xs, train_xs)\n",
    "\n",
    "    # Training loop\n",
    "    running_mistakes = 0\n",
    "    train_accuracy_list = []\n",
    "\n",
    "    converged = False\n",
    "    epoch = 0\n",
    "\n",
    "    while (converged == False) and (epoch <= MAX_EPOCHS):\n",
    "\n",
    "      running_mistakes = 0\n",
    "\n",
    "      # Shuffle the training data\n",
    "      shuffled_indices = [*range(num_examples)]\n",
    "      np.random.shuffle(shuffled_indices)\n",
    "\n",
    "      for example in shuffled_indices:\n",
    "        x = train_xs[example]\n",
    "        y = train_ys[example]\n",
    "\n",
    "        # Predict (highest confidence)\n",
    "        confidences = np.dot(self.alpha, gram_matrix[example, ])\n",
    "        y_hat = np.argmax(confidences)\n",
    "\n",
    "        # Update (if prediction is incorrect)\n",
    "        if (y_hat != y):\n",
    "          running_mistakes += 1\n",
    "        \n",
    "          self.alpha[y, example] += 1 # Raise score of right answer\n",
    "          self.alpha[y_hat, example] -= 1 # Lower score of wrong answer\n",
    "\n",
    "      # Calculate training accuracy\n",
    "      train_accuracy = (num_examples - running_mistakes) / float(num_examples)\n",
    "      train_accuracy_list.append(train_accuracy)\n",
    "\n",
    "      # Early stopping\n",
    "      if epoch >= MIN_EPOCHS:\n",
    "        if (np.mean(train_accuracy_list[-5:]) - np.mean(train_accuracy_list[-10:-5])) < 0.01:\n",
    "          converged = True\n",
    "\n",
    "          # Count number of non-zero alpha vector elements per class\n",
    "          print(f\"Number of non-zero alpha vector elements per class: {np.count_nonzero(self.alpha, axis=1)}\")\n",
    "      \n",
    "      epoch += 1\n",
    "                \n",
    "    return train_accuracy_list\n",
    "\n",
    "  def test(self, test_xs, test_ys):\n",
    "\n",
    "    num_examples = test_xs.shape[0] \n",
    "\n",
    "    # Compute kernel matrix, K\n",
    "    K = self.kernel_function(self.train_xs, test_xs)\n",
    "\n",
    "    running_mistakes = 0\n",
    "    misclassifications_by_example = np.zeros((num_examples))\n",
    "    confusion_matrix = np.zeros((self.num_classes, self.num_classes))\n",
    "\n",
    "    # Make a prediction for each example\n",
    "    for example in range(num_examples):\n",
    "      y = test_ys[example]\n",
    "      y_hat = np.argmax(np.dot(self.alpha, K[:, example]))\n",
    "\n",
    "      # Track mistakes\n",
    "      if (y_hat != y):\n",
    "        running_mistakes += 1\n",
    "        misclassifications_by_example[example] += 1\n",
    "        confusion_matrix[y][y_hat] += 1\n",
    "    \n",
    "    # Calculate test accuracy\n",
    "    test_accuracy = (num_examples - running_mistakes) / float(num_examples)\n",
    "    \n",
    "    return test_accuracy, confusion_matrix, misclassifications_by_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZOYlXQvmi-5f"
   },
   "source": [
    "## 1. Basic results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GZQ4UUUdOxk-"
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10\n",
    "\n",
    "TRAIN_TEST_SPLIT = 0.8\n",
    "\n",
    "DEGREES = range(1, 8)\n",
    "RUNS = 20\n",
    "\n",
    "summary_data = []\n",
    "\n",
    "for degree in DEGREES:\n",
    "\n",
    "  train_accuracy_list = np.empty((RUNS))\n",
    "  test_accuracy_list = np.empty((RUNS))\n",
    "\n",
    "  for run in range(RUNS):\n",
    "\n",
    "    # Generate train/test split\n",
    "    train_xs, train_ys, test_xs, test_ys = split_data(data_xs, data_ys, frac=TRAIN_TEST_SPLIT)\n",
    "\n",
    "    # Initialise model\n",
    "    model = MultiClassPerceptronOvR(NUM_CLASSES)\n",
    "    model.set_polynomial_kernel(degree)\n",
    "\n",
    "    # Train model and evaluate on test set\n",
    "    train_accuracy = model.train(train_xs, train_ys)[-1]\n",
    "    test_accuracy, _, _ = model.test(test_xs, test_ys)\n",
    "\n",
    "    train_accuracy_list[run] = train_accuracy\n",
    "    test_accuracy_list[run] = test_accuracy\n",
    "\n",
    "    print(f\"Polynomial degree: {degree}. Run: {run}. Train accuracy: {train_accuracy}. Test accuracy: {test_accuracy}.\")\n",
    "\n",
    "  # Calculate mean and std for train and test accuracy\n",
    "  train_error_mean, train_error_std = 1 - np.mean(train_accuracy_list), np.std(train_accuracy_list)\n",
    "  test_error_mean, test_error_std = 1 - np.mean(test_accuracy_list), np.std(test_accuracy_list)\n",
    "\n",
    "  summary_data.append([degree, train_error_mean, train_error_std, test_error_mean, test_error_std])\n",
    "  \n",
    "# Create a DataFrame of the summary data\n",
    "summary = pd.DataFrame(summary_data, columns = ('degree', 'train_error_mean', 'train_error_std', 'test_error_mean', 'test_error_std'))\n",
    "\n",
    "# Save summary as CSV\n",
    "csv_filename = f\"Q1_1_{datetime.now().strftime('%Y_%m_%d_%H_%M')}.csv\"\n",
    "summary.to_csv(f\"{path}/{csv_filename}\", index=False, float_format='%.3g')\n",
    "\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vyydrkzxm5yk"
   },
   "source": [
    "## 2. Cross-validation & 3. Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gE02Sh9Q2cCu"
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10\n",
    "\n",
    "TRAIN_TEST_SPLIT = 0.8\n",
    "\n",
    "DEGREES = range(1, 8)\n",
    "RUNS = 20\n",
    "\n",
    "FOLDS = 5\n",
    "\n",
    "summary_data = []\n",
    "confusion_matrices = np.empty((RUNS, NUM_CLASSES, NUM_CLASSES))\n",
    "misclassifications_by_example_acc = np.zeros((data_xs.shape[0]))\n",
    "\n",
    "for run in range(RUNS):\n",
    "\n",
    "  mean_val_accuracy_list = np.empty((len(DEGREES)))\n",
    "\n",
    "  # Generate train/test split\n",
    "  train_xs, train_ys, test_xs, test_ys = split_data(data_xs, data_ys, frac=TRAIN_TEST_SPLIT)\n",
    "\n",
    "  for i, degree in enumerate(DEGREES):\n",
    "\n",
    "    # Generate validation folds\n",
    "    validation_folds = generate_validation_folds(train_xs, train_ys, k=5)\n",
    "\n",
    "    train_accuracy_list = np.empty((FOLDS))\n",
    "    val_accuracy_list = np.empty((FOLDS))\n",
    "\n",
    "    for fold, (fold_train_xs, fold_train_ys, fold_validation_xs, fold_validation_ys) in enumerate(validation_folds):\n",
    "\n",
    "      # Initialise model\n",
    "      model = MultiClassPerceptronOvR(NUM_CLASSES)\n",
    "      model.set_polynomial_kernel(degree=degree)\n",
    "\n",
    "      # Train model and evaluate on validation set\n",
    "      train_accuracy = model.train(fold_train_xs, fold_train_ys)[-1]\n",
    "      val_accuracy, _, _ = model.test(fold_validation_xs, fold_validation_ys)\n",
    "\n",
    "      train_accuracy_list[fold] = train_accuracy\n",
    "      val_accuracy_list[fold] = val_accuracy\n",
    "\n",
    "    print(f\"Polynomial degree: {degree}. Run: {run}. Train accuracy list: {train_accuracy_list}. Val accuracy: {val_accuracy_list}.\")\n",
    "\n",
    "    mean_val_accuracy_list[i] = np.mean(val_accuracy_list)\n",
    "\n",
    "  # Get the degree with maximum mean validation accuracy\n",
    "  argmax = np.argmax(mean_val_accuracy_list)\n",
    "  degree_star = DEGREES[argmax]\n",
    "  \n",
    "  # Initialise model\n",
    "  model = MultiClassPerceptronOvR(NUM_CLASSES)\n",
    "  model.set_polynomial_kernel(degree=degree)\n",
    "\n",
    "  # Train model\n",
    "  train_accuracy = model.train(train_xs, train_ys)[-1]\n",
    "  train_error = 1 - train_accuracy\n",
    "\n",
    "  # Evaluate on test set\n",
    "  test_accuracy, confusion_matrix, _ = model.test(test_xs, test_ys)\n",
    "  test_error = 1 - test_accuracy\n",
    "\n",
    "  # Evaluate on entire data set to help discover the five hardest to predict data items\n",
    "  data_accuracy, _, misclassifications_by_example = model.test(data_xs, data_ys)\n",
    "  misclassifications_by_example_acc += misclassifications_by_example\n",
    "\n",
    "  confusion_matrices[run] = normalize_confusion_matrix(confusion_matrix)\n",
    "  print(confusion_matrices[run])\n",
    "\n",
    "  summary_data.append([degree_star, train_error, test_error])\n",
    "  print(summary_data)\n",
    "\n",
    "# Create a DataFrame of the summary data\n",
    "summary = pd.DataFrame(summary_data, columns = ('degree', 'train_error', 'test_error'))\n",
    "\n",
    "# Create DataFrames of the confusion matrix mean and std\n",
    "confusion_matrix_mean = pd.DataFrame(np.mean(confusion_matrices, axis=0))\n",
    "confusion_matrix_std = pd.DataFrame(np.std(confusion_matrices, axis=0))\n",
    "\n",
    "# Save summary as CSV\n",
    "csv_filename = f\"Q1_2_{datetime.now().strftime('%Y_%m_%d_%H_%M')}_{RUNS}_runs.csv\"\n",
    "summary.to_csv(f\"{ path}/{csv_filename}\", index=False)\n",
    "\n",
    "# Save confusion matrix mean and std as CSV\n",
    "csv_filename = f\"Q1_3_mean_{datetime.now().strftime('%Y_%m_%d_%H_%M')}_{RUNS}_runs.csv\"\n",
    "confusion_matrix_mean.to_csv(f\"{path}/{csv_filename}\", float_format='%.3g')\n",
    " \n",
    "csv_filename = f\"Q1_3_std_{datetime.now().strftime('%Y_%m_%d_%H_%M')}_{RUNS}_runs.csv\"\n",
    "confusion_matrix_std.to_csv(f\"{path}/{csv_filename}\", float_format='%.3g')\n",
    "\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L2cxiDnsOvT8"
   },
   "source": [
    "## 4. Five hardest to predict data items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KfYWZQdaO0fG"
   },
   "outputs": [],
   "source": [
    "# The most misclassified indices & number of times they were misclassified (during RUNS runs)\n",
    "\n",
    "most_misclassified_indices = np.argsort(misclassifications_by_example_acc)[::-1]\n",
    "misclassified_at_least_once = np.count_nonzero(misclassifications_by_example_acc)\n",
    "\n",
    "n_most_misclassified_indices = most_misclassified_indices[:10]\n",
    "n_most_misclassified_counts = misclassifications_by_example_acc[n_most_misclassified_indices]\n",
    "\n",
    "print(\"Data set size:\", len(data_ys))\n",
    "\n",
    "print(f\"Number of distinct examples misclassified over {RUNS} runs: {misclassified_at_least_once} \")\n",
    "\n",
    "print(\"Most misclassified indices:\", n_most_misclassified_indices)\n",
    "print(\"Misclassification counts:\", n_most_misclassified_counts)\n",
    "\n",
    "# Plot the five most frequently misclassified examples\n",
    "for i, example in enumerate(most_misclassified_indices[:5]):\n",
    "  image = np.reshape(data_xs[example], (16, 16))\n",
    "\n",
    "  example_misclassifications = int(misclassifications_by_example_acc[example])\n",
    "\n",
    "  plt.imshow(image, cmap='gray')\n",
    "  plt.axis('off')\n",
    "  plt.title(f\"Label: {data_ys[example]}. Misclassified {example_misclassifications} times.\\n\")\n",
    "  plt.savefig(f\"Q1_4_{i}_most\", dpi=300)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tW9vZB_zNEPJ"
   },
   "source": [
    "## 5. Repeat 1. and 2. with Gaussian kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bElDQPYE6h8J"
   },
   "source": [
    "### 5.a. Basic exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uGMv1c7d6rTQ"
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10\n",
    "\n",
    "TRAIN_TEST_SPLIT = 0.8\n",
    "\n",
    "WIDTHS = [2 ** k for k in range(-15, 10)]\n",
    "RUNS = 20\n",
    "\n",
    "summary_data = []\n",
    "\n",
    "for width in WIDTHS:\n",
    "\n",
    "  train_accuracy_list = np.empty((RUNS))\n",
    "  test_accuracy_list = np.empty((RUNS))\n",
    "\n",
    "  for run in range(RUNS):\n",
    "\n",
    "    # Generate train/test split\n",
    "    train_xs, train_ys, test_xs, test_ys = split_data(data_xs, data_ys, frac=TRAIN_TEST_SPLIT)\n",
    "\n",
    "    # Initialise model\n",
    "    model = MultiClassPerceptronOvR(NUM_CLASSES)\n",
    "    model.set_gaussian_kernel(width)\n",
    "\n",
    "    # Train model and evaluate on test set\n",
    "    train_accuracy = model.train(train_xs, train_ys)[-1]\n",
    "    test_accuracy, _, _ = model.test(test_xs, test_ys)\n",
    "\n",
    "    train_accuracy_list[run] = train_accuracy\n",
    "    test_accuracy_list[run] = test_accuracy\n",
    "\n",
    "    print(f\"Gaussian kernel width: {width}. Run: {run}. Train accuracy: {train_accuracy}. Test accuracy: {test_accuracy}.\")\n",
    "\n",
    "  # Calculate mean and std for train and test accuracy\n",
    "  train_error_mean, train_error_std = 1 - np.mean(train_accuracy_list), np.std(train_accuracy_list)\n",
    "  test_error_mean, test_error_std = 1 - np.mean(test_accuracy_list), np.std(test_accuracy_list)\n",
    "\n",
    "  summary_data.append([width, train_error_mean, train_error_std, test_error_mean, test_error_std])\n",
    "  \n",
    "# Create a DataFrame of the summary data\n",
    "summary = pd.DataFrame(summary_data, columns = ('width', 'train_error_mean', 'train_error_std', 'test_error_mean', 'test_error_std'))\n",
    "\n",
    "# Save summary as CSV\n",
    "csv_filename = f\"Q1_5_a_{datetime.now().strftime('%Y_%m_%d_%H_%M')}.csv\"\n",
    "summary.to_csv(f\"{path}/{csv_filename}\", index=False, float_format='%.3g')\n",
    "\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-gakVwSW6foy"
   },
   "source": [
    "### 5.b. Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dVo_3-UxNBjL"
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10\n",
    "\n",
    "TRAIN_TEST_SPLIT = 0.8\n",
    "\n",
    "WIDTHS = [2 ** k for k in range(-10, -3)]\n",
    "RUNS = 20\n",
    "\n",
    "FOLDS = 5\n",
    "\n",
    "summary_data = []\n",
    "\n",
    "for run in range(RUNS):\n",
    "\n",
    "  mean_val_accuracy_list = np.empty((len(WIDTHS)))\n",
    "\n",
    "  # Generate train/test split\n",
    "  train_xs, train_ys, test_xs, test_ys = split_data(data_xs, data_ys, frac=TRAIN_TEST_SPLIT)\n",
    "\n",
    "  for i, width in enumerate(WIDTHS):\n",
    "\n",
    "    # Generate validation folds\n",
    "    validation_folds = generate_validation_folds(train_xs, train_ys, k=5)\n",
    "\n",
    "    train_accuracy_list = np.empty((FOLDS))\n",
    "    val_accuracy_list = np.empty((FOLDS))\n",
    "\n",
    "    for fold, (fold_train_xs, fold_train_ys, fold_validation_xs, fold_validation_ys) in enumerate(validation_folds):\n",
    "\n",
    "      # Initialise model\n",
    "      model = MultiClassPerceptronOvR(NUM_CLASSES)\n",
    "      model.set_gaussian_kernel(width=width)\n",
    "\n",
    "      # Train model and evaluate on validation set\n",
    "      train_accuracy = model.train(fold_train_xs, fold_train_ys)[-1]\n",
    "      val_accuracy, _, _ = model.test(fold_validation_xs, fold_validation_ys)\n",
    "\n",
    "      train_accuracy_list[fold] = train_accuracy\n",
    "      val_accuracy_list[fold] = val_accuracy\n",
    "\n",
    "    print(f\"Gaussian kernel width: {width}. Run: {run}. Train accuracy list: {train_accuracy_list}. Val accuracy: {val_accuracy_list}.\")\n",
    "\n",
    "    mean_val_accuracy_list[i] = np.mean(val_accuracy_list)\n",
    "\n",
    "  # Get the width with maximum mean validation accuracy\n",
    "  argmax = np.argmax(mean_val_accuracy_list)\n",
    "  width_star = WIDTHS[argmax]\n",
    "  \n",
    "  # Initialise model\n",
    "  model = MultiClassPerceptronOvR(NUM_CLASSES)\n",
    "  model.set_gaussian_kernel(width=width)\n",
    "\n",
    "  # Train model and evaluate on test set\n",
    "  train_accuracy = model.train(train_xs, train_ys)[-1]\n",
    "  test_accuracy, _, _ = model.test(test_xs, test_ys)\n",
    "\n",
    "  train_error = 1 - train_accuracy\n",
    "  test_error = 1 - test_accuracy\n",
    "\n",
    "  summary_data.append([width_star, train_error, test_error])\n",
    "\n",
    "# Create a DataFrame of the summary data\n",
    "summary = pd.DataFrame(summary_data, columns = ('width', 'train_error', 'test_error'))\n",
    "\n",
    "# Save summary as CSV\n",
    "csv_filename = f\"Q1_5_b_{datetime.now().strftime('%Y_%m_%d_%H_%M')}_{RUNS}_runs.csv\"\n",
    "summary.to_csv(f\"{path}/{csv_filename}\", index=False, float_format='%.3g')\n",
    "\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vXVTDXJ0FV-y"
   },
   "source": [
    "## 6. Repeat 1. and 2. with alternative method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tZ7_OAROEE4C"
   },
   "outputs": [],
   "source": [
    "MIN_EPOCHS = 2\n",
    "MAX_EPOCHS = 50\n",
    "\n",
    "class MultiClassPerceptronOvO:\n",
    "  \"\"\"\n",
    "  One-versus-one (OvO) kernel perceptron\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, num_classes):\n",
    "    self.num_classes = num_classes\n",
    "    self.num_classifiers = int(num_classes * (num_classes - 1) / 2)\n",
    "\n",
    "    self.class_combinations = list(combinations(range(num_classes), 2))\n",
    "    self.class_combinations_to_alpha_index = dict(zip(self.class_combinations, range(self.num_classifiers)))\n",
    "\n",
    "    # Initialised by set_polynomial_kernel method\n",
    "    self.kernel_function = None\n",
    "\n",
    "    # Initialised by train method\n",
    "    self.train_xs = None\n",
    "    self.alpha = None\n",
    "\n",
    "  def __get_alpha_index(self, i, j):\n",
    "    \"\"\"\n",
    "    Get the row index for two classes i, j of a classifier in the alpha matrix\n",
    "    \"\"\"\n",
    "    key = tuple(sorted([i, j]))\n",
    "    return self.class_combinations_to_alpha_index[key]\n",
    "  \n",
    "  def set_polynomial_kernel(self, degree):\n",
    "    self.kernel_function = lambda x, y: polynomial_kernel(x, y, degree)\n",
    "\n",
    "  def __predict(self, K_matrix, example):\n",
    "    \"\"\"\n",
    "    Predict the class of an example as the one with the most votes over all\n",
    "    classifiers\n",
    "    \"\"\"\n",
    "    confidences = np.dot(self.alpha, K_matrix[:, example])\n",
    "    binary_predictions = np.sign(confidences)\n",
    "    predicted_class_indices = np.clip(binary_predictions, a_min=0, a_max=None).astype(int)\n",
    "\n",
    "    multiclass_predictions = np.empty((self.num_classifiers)).astype(int)\n",
    "\n",
    "    # Track the prediction of each classifier\n",
    "    for classifier, predicted_class_index in enumerate(predicted_class_indices.tolist()):\n",
    "      multiclass_predictions[classifier] = self.class_combinations[classifier][predicted_class_index]\n",
    "\n",
    "    # Choose the class with the most votes\n",
    "    y_hat = np.bincount(multiclass_predictions).argmax()\n",
    "\n",
    "    return y_hat, multiclass_predictions\n",
    "\n",
    "  def train(self, train_xs, train_ys):\n",
    "\n",
    "    # Initialisation\n",
    "    num_examples = train_xs.shape[0]\n",
    "    self.train_xs = train_xs\n",
    "    self.alpha = np.zeros((self.num_classifiers, num_examples))\n",
    "\n",
    "    # Compute Gram matrix, K\n",
    "    gram_matrix = self.kernel_function(train_xs, train_xs)\n",
    "\n",
    "    # Training loop\n",
    "    running_mistakes = 0\n",
    "    train_accuracy_list = []\n",
    "\n",
    "    converged = False\n",
    "    epoch = 0\n",
    "\n",
    "    while (converged == False) and (epoch <= MAX_EPOCHS):\n",
    "\n",
    "      running_mistakes = 0\n",
    "\n",
    "      # Shuffle the training data\n",
    "      shuffled_indices = [*range(num_examples)]\n",
    "      np.random.shuffle(shuffled_indices)\n",
    "\n",
    "      for example in shuffled_indices:\n",
    "        x = train_xs[example]\n",
    "        y = train_ys[example]\n",
    "        \n",
    "        # Predict (most votes)\n",
    "        y_hat, multiclass_predictions = self.__predict(gram_matrix, example)\n",
    "\n",
    "        # Check if prediction is incorrect\n",
    "        if (y_hat != y):\n",
    "          running_mistakes += 1\n",
    "\n",
    "        # Update relevant (i.e. binary prediction including label y) incorrect classifiers;\n",
    "        # skip if classifier is correct or does not make prediction including label y\n",
    "        for index, (i, j) in enumerate(self.class_combinations_to_alpha_index.keys()):\n",
    "          prediction = multiclass_predictions[index]\n",
    "\n",
    "          if y == i and prediction != y: # If so, y is the negative class for an incorrect classifier \n",
    "            self.alpha[index, example] -= 1\n",
    "          elif y == j and prediction != y: # If so, y is the positive class for an incorrect classifier\n",
    "            self.alpha[index, example] += 1\n",
    "        \n",
    "      # Calculate training accuracy\n",
    "      train_accuracy = (num_examples - running_mistakes) / float(num_examples)\n",
    "      train_accuracy_list.append(train_accuracy)\n",
    "\n",
    "      # Early stopping\n",
    "      if epoch >= MIN_EPOCHS:\n",
    "        if (np.mean(train_accuracy_list[-2:]) - np.mean(train_accuracy_list[-4:-2])) < 0.01:\n",
    "          converged = True\n",
    "\n",
    "          # Count number of non-zero alpha vector elements per class\n",
    "          print(f\"Number of non-zero alpha vector elements per class: {np.count_nonzero(self.alpha, axis=1)}\")\n",
    "      \n",
    "      epoch += 1\n",
    "                \n",
    "    return train_accuracy_list\n",
    "\n",
    "  def test(self, test_xs, test_ys):\n",
    "\n",
    "    num_examples = test_xs.shape[0]\n",
    "\n",
    "    # Compute kernel matrix, K\n",
    "    K_matrix = self.kernel_function(self.train_xs, test_xs)\n",
    "\n",
    "    running_mistakes = 0\n",
    "    confusion_matrix = np.zeros((self.num_classes, self.num_classes))\n",
    "\n",
    "    # Make a prediction for each example\n",
    "    for example in range(num_examples):\n",
    "      y = test_ys[example]\n",
    "\n",
    "      # Predict (most votes)\n",
    "      y_hat, _ = self.__predict(K_matrix, example)\n",
    "\n",
    "      # Track mistakes\n",
    "      if (y_hat != y):\n",
    "        running_mistakes += 1\n",
    "        confusion_matrix[y][y_hat] += 1\n",
    "    \n",
    "    # Calculate test accuracy\n",
    "    test_accuracy = (num_examples - running_mistakes) / float(num_examples)\n",
    "    \n",
    "    return test_accuracy, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cpAZMyosFYIc"
   },
   "source": [
    "### 6.a. Basic results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j5UhnLVihTXC"
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10\n",
    "\n",
    "TRAIN_TEST_SPLIT = 0.8\n",
    "\n",
    "DEGREES = range(1, 8)\n",
    "RUNS = 20\n",
    "\n",
    "summary_data = []\n",
    "\n",
    "for degree in DEGREES:\n",
    "\n",
    "  train_accuracy_list = np.empty((RUNS))\n",
    "  test_accuracy_list = np.empty((RUNS))\n",
    "\n",
    "  for run in range(RUNS):\n",
    "\n",
    "    # Generate train/test split\n",
    "    train_xs, train_ys, test_xs, test_ys = split_data(data_xs, data_ys, frac=TRAIN_TEST_SPLIT)\n",
    "\n",
    "    # Initialise model\n",
    "    model = MultiClassPerceptronOvO(NUM_CLASSES)\n",
    "    model.set_polynomial_kernel(degree)\n",
    "\n",
    "    # Train model and evaluate on test set\n",
    "    train_accuracy = model.train(train_xs, train_ys)[-1]\n",
    "    test_accuracy, _ = model.test(test_xs, test_ys)\n",
    "\n",
    "    train_accuracy_list[run] = train_accuracy\n",
    "    test_accuracy_list[run] = test_accuracy\n",
    "\n",
    "    print(f\"Polynomial degree: {degree}. Run: {run}. Train accuracy: {train_accuracy}. Test accuracy: {test_accuracy}.\")\n",
    "\n",
    "  # Calculate mean and std for train and test accuracy\n",
    "  train_error_mean, train_error_std = 1 - np.mean(train_accuracy_list), np.std(train_accuracy_list)\n",
    "  test_error_mean, test_error_std = 1 - np.mean(test_accuracy_list), np.std(test_accuracy_list)\n",
    "\n",
    "  summary_data.append([degree, train_error_mean, train_error_std, test_error_mean, test_error_std])\n",
    "  \n",
    "# Create a DataFrame of the summary data\n",
    "summary = pd.DataFrame(summary_data, columns = ('degree', 'train_error_mean', 'train_error_std', 'test_error_mean', 'test_error_std'))\n",
    "\n",
    "# Save summary as CSV\n",
    "csv_filename = f\"Q1_6_a_{datetime.now().strftime('%Y_%m_%d_%H_%M')}.csv\"\n",
    "summary.to_csv(f\"{path}/{csv_filename}\", index=False, float_format='%.3g')\n",
    "\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SiDcVNBGFfGr"
   },
   "source": [
    "### 6.b. Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eto178gyhS14"
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10\n",
    "\n",
    "TRAIN_TEST_SPLIT = 0.8\n",
    "\n",
    "DEGREES = range(1, 8)\n",
    "RUNS = 20\n",
    "\n",
    "FOLDS = 5\n",
    "\n",
    "summary_data = []\n",
    "confusion_matrices = np.empty((RUNS, NUM_CLASSES, NUM_CLASSES))\n",
    "\n",
    "for run in range(RUNS):\n",
    "\n",
    "  mean_val_accuracy_list = np.empty((len(DEGREES)))\n",
    "\n",
    "  # Generate train/test split\n",
    "  train_xs, train_ys, test_xs, test_ys = split_data(data_xs, data_ys, frac=TRAIN_TEST_SPLIT)\n",
    "\n",
    "  for i, degree in enumerate(DEGREES):\n",
    "\n",
    "    # Generate validation folds\n",
    "    validation_folds = generate_validation_folds(train_xs, train_ys, k=5)\n",
    "\n",
    "    train_accuracy_list = np.empty((FOLDS))\n",
    "    val_accuracy_list = np.empty((FOLDS))\n",
    "\n",
    "    for fold, (fold_train_xs, fold_train_ys, fold_validation_xs, fold_validation_ys) in enumerate(validation_folds):\n",
    "\n",
    "      # Initialise model\n",
    "      model = MultiClassPerceptronOvO(NUM_CLASSES)\n",
    "      model.set_polynomial_kernel(degree=degree)\n",
    "\n",
    "      # Train model and evaluate on validation set\n",
    "      train_accuracy = model.train(fold_train_xs, fold_train_ys)[-1]\n",
    "      val_accuracy, _ = model.test(fold_validation_xs, fold_validation_ys)\n",
    "\n",
    "      train_accuracy_list[fold] = train_accuracy\n",
    "      val_accuracy_list[fold] = val_accuracy\n",
    "\n",
    "    print(f\"Polynomial degree: {degree}. Run: {run}. Train accuracy list: {train_accuracy_list}. Val accuracy: {val_accuracy_list}.\")\n",
    "\n",
    "    mean_val_accuracy_list[i] = np.mean(val_accuracy_list)\n",
    "\n",
    "  # Get the degree with maximum mean validation accuracy\n",
    "  argmax = np.argmax(mean_val_accuracy_list)\n",
    "  degree_star = DEGREES[argmax]\n",
    "  \n",
    "  # Initialise model\n",
    "  model = MultiClassPerceptronOvO(NUM_CLASSES)\n",
    "  model.set_polynomial_kernel(degree=degree)\n",
    "\n",
    "  # Train model and evaluate on test set\n",
    "  train_accuracy = model.train(train_xs, train_ys)[-1]\n",
    "  test_accuracy, _ = model.test(test_xs, test_ys)\n",
    "\n",
    "  train_error = 1 - train_accuracy\n",
    "  test_error = 1 - test_accuracy\n",
    "\n",
    "  summary_data.append([degree_star, train_error, test_error])\n",
    "  print(summary_data)\n",
    "\n",
    "# Create a DataFrame of the summary data\n",
    "summary = pd.DataFrame(summary_data, columns = ('degree', 'train_error', 'test_error'))\n",
    "\n",
    "# Save summary as CSV\n",
    "csv_filename = f\"Q1_6_b_{datetime.now().strftime('%Y_%m_%d_%H_%M')}_{RUNS}_runs.csv\"\n",
    "summary.to_csv(f\"{path}/{csv_filename}\", index=False)\n",
    "\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9MvZJgnchEPY"
   },
   "source": [
    "## 7. Comparison algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uLh837-OhMEb"
   },
   "source": [
    "### 7.a. Kernel SVM with CVXOPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3xKio0MD_UO9"
   },
   "outputs": [],
   "source": [
    "class BinarySVM:\n",
    "  \"\"\"\n",
    "  Binary kernel support vector machine (SVM)\n",
    "  \"\"\"\n",
    "  def __init__(self, C = 1.0):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      C: The amount of regularization\n",
    "    \"\"\"\n",
    "\n",
    "    self.C = C\n",
    "\n",
    "    # Initialised by set_{polynomial/gaussian}_kernel method\n",
    "    self.kernel_function = None\n",
    "\n",
    "    # Initialised by train method\n",
    "    self.alpha = None\n",
    "    self.support_vectors = None\n",
    "    self.support_labels = None\n",
    "    self.bias = None\n",
    "  \n",
    "  def set_polynomial_kernel(self, degree):\n",
    "    self.kernel_function = lambda x, y: polynomial_kernel(x, y, degree)\n",
    "  \n",
    "  def set_gaussian_kernel(self, width):\n",
    "    self.kernel_function = lambda p, q: gaussian_kernel(p, q, width)\n",
    "\n",
    "  def train(self, train_xs, train_ys):\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Initialisation\n",
    "    num_examples = train_xs.shape[0]\n",
    "\n",
    "    # Compute Gram matrix, K\n",
    "    K = self.kernel_function(train_xs, train_xs)\n",
    "\n",
    "    # Formulate quadratic programming problem\n",
    "\n",
    "    # (1/2) x.T P x + q.T x\n",
    "    P = matrix(np.outer(train_ys, train_ys) * K)\n",
    "    q = matrix(np.repeat(-1., num_examples))\n",
    "    # Ax = b\n",
    "    A = matrix(train_ys.astype(np.double), (1, num_examples))\n",
    "    b = matrix(0.)\n",
    "    # Gx <= h\n",
    "    G = matrix(np.vstack((-np.eye(num_examples),\n",
    "                           np.eye(num_examples))))\n",
    "    h = matrix(np.hstack((np.zeros(num_examples),\n",
    "                          np.ones(num_examples) * self.C)))\n",
    "    \n",
    "    print(f\"Formulated {time.time() - start}\")\n",
    "\n",
    "    # Solve\n",
    "    solvers.options['show_progress'] = False\n",
    "    solution = solvers.qp(P, q, G, h, A, b)\n",
    "\n",
    "    print(f\"Solved {time.time() - start}\")\n",
    "\n",
    "    # Multipliers\n",
    "    multipliers = np.ravel(solution['x'])\n",
    "    \n",
    "    # Support\n",
    "    is_support_vector = multipliers > 1e-5\n",
    "    support_vector_indices = np.flatnonzero(is_support_vector)\n",
    "    \n",
    "    self.alpha = multipliers[support_vector_indices]\n",
    "    self.support_vectors = train_xs[support_vector_indices]\n",
    "    self.support_labels = train_ys[support_vector_indices]\n",
    "\n",
    "    # Bias\n",
    "    self.bias = 0\n",
    "    for i in range(len(self.alpha)):\n",
    "      self.bias += self.support_labels[i]\n",
    "      self.bias -= np.sum(self.alpha * self.support_labels * K[support_vector_indices[i], is_support_vector])\n",
    "    self.bias /= len(self.alpha)\n",
    "\n",
    "    return self\n",
    "\n",
    "  def project(self, test_xs, test_ys):\n",
    "\n",
    "    # Compute K matrix\n",
    "    K = self.kernel_function(self.support_vectors, test_xs)\n",
    "\n",
    "    num_examples = test_xs.shape[0]\n",
    "\n",
    "    projections = np.empty((num_examples))\n",
    "\n",
    "    for example in range(num_examples):\n",
    "      projections[example] = self.bias + np.sum(self.alpha * self.support_labels * K[:, example])\n",
    "\n",
    "    return projections\n",
    "  \n",
    "  def test(self, test_xs, test_ys):\n",
    "    \n",
    "    predictions = np.sign(self.project(test_xs, test_ys))\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z6kKpLBKhREr"
   },
   "outputs": [],
   "source": [
    "class MulticlassSVM:\n",
    "  \"\"\"\n",
    "  One-versus-rest (OvR) kernel support vector machine (SVM)\n",
    "  \"\"\"\n",
    "  def __init__(self, num_classes, C = 1.0):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      num_classes: The number of classes\n",
    "      C: The amount of regularization\n",
    "    \"\"\"\n",
    "\n",
    "    self.num_classes = num_classes\n",
    "    self.classifiers = [BinarySVM(C) for i in range(num_classes)]\n",
    "\n",
    "  def set_polynomial_kernel(self, degree):\n",
    "    for cfr in self.classifiers:\n",
    "      cfr.set_polynomial_kernel(degree)\n",
    "  \n",
    "  def set_gaussian_kernel(self, width):\n",
    "    for cfr in self.classifiers:\n",
    "      cfr.set_gaussian_kernel(width)\n",
    "  \n",
    "  def train(self, train_xs, train_ys):\n",
    "\n",
    "    # Training loop\n",
    "    for i in range(self.num_classes):\n",
    "      \n",
    "      # Uncomment to use all training examples:\n",
    "      # train_ys_i = np.where(train_ys == i, 1, -1)\n",
    "      # self.classifiers[i].train(train_xs, train_ys_i)\n",
    "\n",
    "      # Uncomment to use subsampling method:\n",
    "      # Uses all training examples in class i, and a randomly selected set of\n",
    "      # class non-i of the same size\n",
    "\n",
    "      # Adjust labels to +1 for class i and -1 otherwise\n",
    "      train_ys_i = np.where(train_ys == i, 1, -1)\n",
    "\n",
    "      # Get indices of class i examples\n",
    "      class_i_indices = np.where(train_ys_i == 1)\n",
    "      class_i_num_examples = len(class_i_indices[0])\n",
    "\n",
    "      # Get class_i_num_examples indices of class non-i examples at random\n",
    "      non_class_i_indices = np.random.choice(np.where(train_ys_i == -1)[0], class_i_num_examples, replace=False)\n",
    "\n",
    "      # Combine the i and non-i classes and randomly shuffle\n",
    "      subset_indices = np.append(class_i_indices, non_class_i_indices)\n",
    "      np.random.shuffle(subset_indices)\n",
    "\n",
    "      train_xs_subset = train_xs[subset_indices]\n",
    "      train_ys_subset = train_ys_i[subset_indices]\n",
    "\n",
    "      self.classifiers[i].train(train_xs_subset, train_ys_subset)\n",
    "\n",
    "    support_vectors_per_class = list(map(lambda clf: np.count_nonzero(clf.alpha), self.classifiers))\n",
    "    print(f\"Number of support vectors per class: {support_vectors_per_class}\")\n",
    "\n",
    "    return self\n",
    "\n",
    "  def test(self, test_xs, test_ys):\n",
    "    \n",
    "    num_examples = test_xs.shape[0]\n",
    "\n",
    "    projections = np.empty((self.num_classes, num_examples))\n",
    "\n",
    "    # Compute projections for each classifier\n",
    "    for i in range(self.num_classes):\n",
    "      cfr = self.classifiers[i]\n",
    "      projections[i] = cfr.project(test_xs, test_ys)\n",
    "    \n",
    "    # Predict the classes with the largest projections\n",
    "    predictions = np.argmax(projections, axis=0)\n",
    "\n",
    "    # Compute test accuracy\n",
    "    test_accuracy = sum(predictions == test_ys) / num_examples\n",
    "\n",
    "    return test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yK1Wgk2b_YmO"
   },
   "outputs": [],
   "source": [
    "# 1. Basic results\n",
    "\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "TRAIN_TEST_SPLIT = 0.8\n",
    "\n",
    "C_VALUES = [2 ** k for k in range(-1, 4)]\n",
    "WIDTHS = [2 ** k for k in range(-7, -4)]\n",
    "\n",
    "HYPERPARAMETERS = [(C, width) for C in C_VALUES for width in WIDTHS]\n",
    "\n",
    "RUNS = 20\n",
    "\n",
    "summary_data = []\n",
    "\n",
    "for (C, width) in HYPERPARAMETERS:\n",
    "\n",
    "  train_accuracy_list = np.empty((RUNS))\n",
    "  test_accuracy_list = np.empty((RUNS))\n",
    "\n",
    "  for run in range(RUNS):\n",
    "\n",
    "    # Generate train/test split\n",
    "    train_xs, train_ys, test_xs, test_ys = split_data(data_xs, data_ys, frac=TRAIN_TEST_SPLIT)\n",
    "\n",
    "    # Initialise model\n",
    "    model = MulticlassSVM(num_classes = 10, C = C)\n",
    "    model.set_gaussian_kernel(width)\n",
    "\n",
    "    # Train model\n",
    "    model.train(train_xs, train_ys)\n",
    "\n",
    "    # Evaluate on training and test sets\n",
    "    train_accuracy = model.test(train_xs, train_ys)\n",
    "    test_accuracy = model.test(test_xs, test_ys)\n",
    "\n",
    "    train_accuracy_list[run] = train_accuracy\n",
    "    test_accuracy_list[run] = test_accuracy\n",
    "\n",
    "    print(f\"C: {C}. Gaussian kernel width: {width}. Run: {run}. Train accuracy: {train_accuracy}. Test accuracy: {test_accuracy}.\")\n",
    "\n",
    "  # Calculate mean and std for train and test accuracy\n",
    "  train_error_mean, train_error_std = 1 - np.mean(train_accuracy_list), np.std(train_accuracy_list)\n",
    "  test_error_mean, test_error_std = 1 - np.mean(test_accuracy_list), np.std(test_accuracy_list)\n",
    "\n",
    "  summary_data.append([C, width, train_error_mean, train_error_std, test_error_mean, test_error_std])\n",
    "  \n",
    "# Create a DataFrame of the summary data\n",
    "summary = pd.DataFrame(summary_data, columns = ('C', 'width', 'train_error_mean', 'train_error_std', 'test_error_mean', 'test_error_std'))\n",
    "\n",
    "# Save summary as CSV\n",
    "csv_filename = f\"Q1_7a_1_{datetime.now().strftime('%Y_%m_%d_%H_%M')}.csv\"\n",
    "summary.to_csv(f\"{path}/{csv_filename}\", index=False, float_format='%.3g')\n",
    "\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OvH_sugbOkWH"
   },
   "outputs": [],
   "source": [
    "# 2. Cross-validation\n",
    "\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "TRAIN_TEST_SPLIT = 0.8\n",
    "\n",
    "C_VALUES = [2 ** k for k in range(-1, 4)]\n",
    "WIDTHS = [2 ** k for k in range(-7, -4)]\n",
    "\n",
    "HYPERPARAMETERS = [(C, width) for C in C_VALUES for width in WIDTHS]\n",
    "\n",
    "RUNS = 20\n",
    "\n",
    "FOLDS = 5\n",
    "\n",
    "summary_data = []\n",
    "\n",
    "for run in range(RUNS):\n",
    "\n",
    "  mean_val_accuracy_list = np.empty((len(HYPERPARAMETERS)))\n",
    "\n",
    "  # Generate train/test split\n",
    "  train_xs, train_ys, test_xs, test_ys = split_data(data_xs, data_ys, frac=TRAIN_TEST_SPLIT)\n",
    "\n",
    "  for i, (C, width) in enumerate(HYPERPARAMETERS):\n",
    "\n",
    "    # Generate validation folds\n",
    "    validation_folds = generate_validation_folds(train_xs, train_ys, k=5)\n",
    "\n",
    "    train_accuracy_list = np.empty((FOLDS))\n",
    "    val_accuracy_list = np.empty((FOLDS))\n",
    "\n",
    "    for fold, (fold_train_xs, fold_train_ys, fold_validation_xs, fold_validation_ys) in enumerate(validation_folds):\n",
    "\n",
    "      # Initialise model\n",
    "      model = MulticlassSVM(NUM_CLASSES, C=C)\n",
    "      model.set_gaussian_kernel(width=width)\n",
    "\n",
    "      # Train model\n",
    "      model.train(fold_train_xs, fold_train_ys)\n",
    "\n",
    "      # Evaluate on training and validation sets\n",
    "      train_accuracy = model.test(fold_train_xs, fold_train_ys)\n",
    "      val_accuracy = model.test(fold_validation_xs, fold_validation_ys)\n",
    "\n",
    "      train_accuracy_list[fold] = train_accuracy\n",
    "      val_accuracy_list[fold] = val_accuracy\n",
    "\n",
    "    print(f\"C: {C}. Gaussian kernel width: {width}. Run: {run}. Train accuracy list: {train_accuracy_list}. Val accuracy: {val_accuracy_list}.\")\n",
    "\n",
    "    mean_val_accuracy_list[i] = np.mean(val_accuracy_list)\n",
    "\n",
    "  # Get the width with maximum mean validation accuracy\n",
    "  argmax = np.argmax(mean_val_accuracy_list)\n",
    "  (C_star, width_star) = HYPERPARAMETERS[argmax]\n",
    "  \n",
    "  # Initialise model\n",
    "  model = MulticlassSVM(NUM_CLASSES, C=C_star)\n",
    "  model.set_gaussian_kernel(width=width_star)\n",
    "\n",
    "  # Train model\n",
    "  model.train(train_xs, train_ys)\n",
    "\n",
    "  # Evaluate on training and test sets\n",
    "  train_accuracy = model.test(train_xs, train_ys)\n",
    "  test_accuracy = model.test(test_xs, test_ys)\n",
    "\n",
    "  train_error = 1 - train_accuracy\n",
    "  test_error = 1 - test_accuracy\n",
    "\n",
    "  summary_data.append([C_star, width_star, train_error, test_error])\n",
    "\n",
    "# Create a DataFrame of the summary data\n",
    "summary = pd.DataFrame(summary_data, columns = ('C', 'width', 'train_error', 'test_error'))\n",
    "\n",
    "# Save summary as CSV\n",
    "csv_filename = f\"Q1_7a_2_{datetime.now().strftime('%Y_%m_%d_%H_%M')}_{RUNS}_runs.csv\"\n",
    "summary.to_csv(f\"{path}/{csv_filename}\", index=False, float_format='%.3g')\n",
    "\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nydot5OQP5wO"
   },
   "source": [
    "### 7.a. Kernel SVM (one-versus-rest) with SMO solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-nkL3F2mQE5n"
   },
   "outputs": [],
   "source": [
    "class BinarySVM_SMO:\n",
    "  \"\"\"\n",
    "  Binary kernel support vector machine (SVM) using sequential minimal optimization\n",
    "  \"\"\"\n",
    "  def __init__(self, K, C = 1.0, tol=1e-5, max_iters=100):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      K: The Gram matrix for the training examples\n",
    "      C: The amount of regularization\n",
    "      tol: Numerical tolerance for Lagrange multipliers\n",
    "      max_iters = The maximum number of passes over the Lagrange multipliers\n",
    "    \"\"\"\n",
    "\n",
    "    self.K = K\n",
    "    self.C = C\n",
    "    self.tol = tol\n",
    "    self.max_iters = max_iters\n",
    "    self.max_iters_no_updates = 2\n",
    "\n",
    "    self.num_examples = train_xs.shape[0]\n",
    "\n",
    "    # Initialise\n",
    "    self.alpha = np.zeros((self.num_examples))\n",
    "    self.bias = 0\n",
    "\n",
    "    # Initialised by set_gaussian_kernel\n",
    "    self.kernel_function = None\n",
    "\n",
    "    # Initialised by train method\n",
    "    self.support_vectors = None\n",
    "    self.support_labels = None\n",
    "    self.train_xs = None\n",
    "    self.train_ys = None\n",
    "\n",
    "  def set_gaussian_kernel(self, width):\n",
    "    self.kernel_function = lambda p, q: gaussian_kernel(p, q, width)\n",
    "\n",
    "  def __calculate_E(self, index):\n",
    "    \"\"\"\n",
    "    Calculate f(x_i) - y_i\n",
    "    \"\"\"\n",
    "    f_x_index = np.dot((self.alpha * self.train_ys).T, self.K[:, index]) + self.bias\n",
    "    return f_x_index - self.train_ys[index]\n",
    "\n",
    "  def __randint_neq(self, start, stop, i):\n",
    "    \"\"\"\n",
    "    Generate a random number between start and stop, that is not equal to i\n",
    "    \"\"\"\n",
    "    j = randint(start, stop)\n",
    "    while i == j:\n",
    "      j = randint(start, stop)\n",
    "    return j\n",
    "\n",
    "  def train(self, train_xs, train_ys):\n",
    "\n",
    "    self.train_xs = train_xs\n",
    "    self.train_ys = train_ys\n",
    "\n",
    "    # Track the number of consecutive iterations without updates, and the total number\n",
    "    # of iterations\n",
    "    iters_no_updates = 0\n",
    "    iters = 0\n",
    "\n",
    "    # Loop until no updates max_iters_no_updates times in a row, or reaches max_iters\n",
    "    while(iters_no_updates < self.max_iters_no_updates and iters < self.max_iters):\n",
    "      \n",
    "      num_changed_alphas = 0\n",
    "      \n",
    "      for i in range(self.num_examples):\n",
    "        \n",
    "        y_i = self.train_ys[i]\n",
    "        E_i = self.__calculate_E(i)\n",
    "\n",
    "        if (y_i * E_i < -self.tol and self.alpha[i] < self.C) or \\\n",
    "          (y_i * E_i > self.tol and self.alpha[i] > 0):\n",
    "\n",
    "          # Select random j != i\n",
    "          j = self.__randint_neq(0, self.num_examples - 1, i)\n",
    "\n",
    "          y_j = self.train_ys[j]\n",
    "          E_j = self.__calculate_E(j)\n",
    "\n",
    "          # Save old alphas\n",
    "          alpha_i_old, alpha_j_old = self.alpha[i], self.alpha[j]\n",
    "\n",
    "          if y_i != y_j:\n",
    "            L = max(0, self.alpha[j] - self.alpha[i])\n",
    "            H = min(self.C, self.C + self.alpha[j] - self.alpha[i])\n",
    "          else:\n",
    "            L = max(0, self.alpha[i] + self.alpha[j] - self.C)\n",
    "            H = min(C, self.alpha[i] + self.alpha[j])\n",
    "\n",
    "          if L == H:\n",
    "            continue # next i\n",
    "          \n",
    "          eta = 2 * self.K[i, j] - self.K[i, i] - self.K[j ,j]\n",
    "\n",
    "          if (eta >= 0):\n",
    "            continue # next i\n",
    "          \n",
    "          # Clip alpha_j\n",
    "          self.alpha[j] = alpha_j_old - (y_j*(E_i - E_j))/eta\n",
    "          self.alpha[j] = min(self.alpha[j], H)\n",
    "          self.alpha[j] = max(self.alpha[j], L)\n",
    "\n",
    "          if abs(self.alpha[j] - alpha_j_old) < self.tol:\n",
    "            continue # next i\n",
    "          \n",
    "          # Solve for alpha_i\n",
    "          self.alpha[i] = alpha_i_old + y_i * y_j * (alpha_j_old - self.alpha[j])\n",
    "\n",
    "          # Compute the bias\n",
    "          b_1 = self.bias - E_i - y_i * (self.alpha[i] - alpha_i_old) * self.K[i, i] - y_j * (self.alpha[j] - alpha_j_old) * self.K[i, j]\n",
    "          b_2 = self.bias - E_j - y_i * (self.alpha[i] - alpha_i_old) * self.K[i, j] - y_j * (self.alpha[j] - alpha_j_old) * self.K[j, j]\n",
    "\n",
    "          if 0 < self.alpha[i] < self.C:\n",
    "            self.bias = b_1 \n",
    "          if 0 < self.alpha[j] < self.C:\n",
    "            self.bias = b_2\n",
    "          else:\n",
    "            self.bias = (b_1 + b_2) / 2\n",
    "          \n",
    "          num_changed_alphas += 1\n",
    "      \n",
    "      iters += 1\n",
    "\n",
    "      if num_changed_alphas == 0:\n",
    "        iters_no_updates += 1\n",
    "      else:\n",
    "        iters_no_updates = 0\n",
    "\n",
    "    # Support\n",
    "    is_support_vector = self.alpha > 0\n",
    "    support_vector_indices = np.flatnonzero(is_support_vector)\n",
    "      \n",
    "    self.alpha = self.alpha[support_vector_indices]\n",
    "    self.support_vectors = train_xs[support_vector_indices]\n",
    "    self.support_labels = train_ys[support_vector_indices]\n",
    "\n",
    "  def project(self, test_xs, test_ys):\n",
    "\n",
    "    # Compute K matrix\n",
    "    K = self.kernel_function(self.support_vectors, test_xs)\n",
    "\n",
    "    num_examples = test_xs.shape[0]\n",
    "\n",
    "    projections = np.empty((num_examples))\n",
    "\n",
    "    # Project each example\n",
    "    for example in range(num_examples):\n",
    "      projections[example] = self.bias + np.sum(self.alpha * self.support_labels * K[:, example])\n",
    "\n",
    "    return projections\n",
    "  \n",
    "  def test(self, test_xs, test_ys):\n",
    "    \n",
    "    predictions = np.sign(self.project(test_xs, test_ys))\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o7sO1skDfcyH"
   },
   "outputs": [],
   "source": [
    "class MulticlassSVM_SMO:\n",
    "  \"\"\"\n",
    "  One-versus-rest (OvR) Gaussian kernel support vector machine (SVM) using\n",
    "  sequential minimal optimization\n",
    "  \"\"\"\n",
    "  def __init__(self, train_xs, train_ys, width, num_classes, C = 1.0):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      train_xs: The training examples\n",
    "      train_ys: The training labels\n",
    "      width: The Gaussian kernel width\n",
    "      num_classes: The number of classes\n",
    "      C: The amount of regularization\n",
    "    \"\"\"\n",
    "\n",
    "    self.num_classes = num_classes\n",
    "\n",
    "    # Compute Gram matrix, K\n",
    "    K = gaussian_kernel(train_xs, train_xs, width)\n",
    "\n",
    "    self.classifiers = [BinarySVM_SMO(K, C) for i in range(num_classes)]\n",
    "\n",
    "    for cfr in self.classifiers:\n",
    "      cfr.set_gaussian_kernel(width)\n",
    "  \n",
    "  def train(self):\n",
    "\n",
    "    # Training loop\n",
    "    for i in range(self.num_classes):\n",
    "\n",
    "      print(\"Training:\", i)\n",
    "      train_ys_i = np.where(train_ys == i, 1, -1)\n",
    "      self.classifiers[i].train(train_xs, train_ys_i)\n",
    "\n",
    "    support_vectors_per_class = list(map(lambda clf: np.count_nonzero(clf.alpha), self.classifiers))\n",
    "    print(f\"Number of support vectors per class: {support_vectors_per_class}\")\n",
    "\n",
    "    return self\n",
    "\n",
    "  def test(self, test_xs, test_ys):\n",
    "    \n",
    "    num_examples = test_xs.shape[0]\n",
    "\n",
    "    projections = np.empty((self.num_classes, num_examples))\n",
    "\n",
    "    # Compute projections for each classifier\n",
    "    for i in range(self.num_classes):\n",
    "      cfr = self.classifiers[i]\n",
    "      projections[i] = cfr.project(test_xs, test_ys)\n",
    "    \n",
    "    # Predict the classes with the largest projections\n",
    "    predictions = np.argmax(projections, axis=0)\n",
    "\n",
    "    # Compute test accuracy\n",
    "    test_accuracy = sum(predictions == test_ys) / num_examples\n",
    "\n",
    "    return test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wtz9sim6fqZR"
   },
   "outputs": [],
   "source": [
    "# 1. Basic results\n",
    "\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "TRAIN_TEST_SPLIT = 0.8\n",
    "\n",
    "C_VALUES = [2 ** k for k in range(-1, 4)]\n",
    "WIDTHS = [2 ** k for k in range(-7, -4)]\n",
    "\n",
    "HYPERPARAMETERS = [(C, width) for C in C_VALUES for width in WIDTHS]\n",
    "\n",
    "RUNS = 20\n",
    "\n",
    "summary_data = []\n",
    "\n",
    "for (C, width) in HYPERPARAMETERS:\n",
    "\n",
    "  train_accuracy_list = np.empty((RUNS))\n",
    "  test_accuracy_list = np.empty((RUNS))\n",
    "\n",
    "  for run in range(RUNS):\n",
    "\n",
    "    # Generate train/test split\n",
    "    train_xs, train_ys, test_xs, test_ys = split_data(data_xs, data_ys, frac=TRAIN_TEST_SPLIT)\n",
    "\n",
    "    # Initialise model\n",
    "    model = MulticlassSVM_SMO(train_xs, train_ys, width, num_classes = 10, C = C)\n",
    "\n",
    "    # Train model\n",
    "    model.train()\n",
    "\n",
    "    # Evaluate on training and test sets\n",
    "    train_accuracy = model.test(train_xs, train_ys)\n",
    "    test_accuracy = model.test(test_xs, test_ys)\n",
    "\n",
    "    train_accuracy_list[run] = train_accuracy\n",
    "    test_accuracy_list[run] = test_accuracy\n",
    "\n",
    "    print(f\"C: {C}. Gaussian kernel width: {width}. Run: {run}. Train accuracy: {train_accuracy}. Test accuracy: {test_accuracy}.\")\n",
    "\n",
    "  # Calculate mean and std for train and test accuracy\n",
    "  train_error_mean, train_error_std = 1 - np.mean(train_accuracy_list), np.std(train_accuracy_list)\n",
    "  test_error_mean, test_error_std = 1 - np.mean(test_accuracy_list), np.std(test_accuracy_list)\n",
    "\n",
    "  summary_data.append([C, width, train_error_mean, train_error_std, test_error_mean, test_error_std])\n",
    "  \n",
    "# Create a DataFrame of the summary data\n",
    "summary = pd.DataFrame(summary_data, columns = ('C', 'width', 'train_error_mean', 'train_error_std', 'test_error_mean', 'test_error_std'))\n",
    "\n",
    "# Save summary as CSV\n",
    "csv_filename = f\"Q1_7a_smo_1_{datetime.now().strftime('%Y_%m_%d_%H_%M')}.csv\"\n",
    "summary.to_csv(f\"{path}/{csv_filename}\", index=False, float_format='%.3g')\n",
    "\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-EICf6tqWKfY"
   },
   "outputs": [],
   "source": [
    "# 2. Cross-validation\n",
    "\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "TRAIN_TEST_SPLIT = 0.8\n",
    "\n",
    "C_VALUES = [2 ** k for k in range(-1, 4)]\n",
    "WIDTHS = [2 ** k for k in range(-7, -4)]\n",
    "\n",
    "HYPERPARAMETERS = [(C, width) for C in C_VALUES for width in WIDTHS]\n",
    "\n",
    "RUNS = 20\n",
    "\n",
    "FOLDS = 5\n",
    "\n",
    "summary_data = []\n",
    "\n",
    "for run in range(RUNS):\n",
    "\n",
    "  mean_val_accuracy_list = np.empty((len(HYPERPARAMETERS)))\n",
    "\n",
    "  # Generate train/test split\n",
    "  train_xs, train_ys, test_xs, test_ys = split_data(data_xs, data_ys, frac=TRAIN_TEST_SPLIT)\n",
    "\n",
    "  for i, (C, width) in enumerate(HYPERPARAMETERS):\n",
    "\n",
    "    # Generate validation folds\n",
    "    validation_folds = generate_validation_folds(train_xs, train_ys, k=5)\n",
    "\n",
    "    train_accuracy_list = np.empty((FOLDS))\n",
    "    val_accuracy_list = np.empty((FOLDS))\n",
    "\n",
    "    for fold, (fold_train_xs, fold_train_ys, fold_validation_xs, fold_validation_ys) in enumerate(validation_folds):\n",
    "\n",
    "      # Initialise model\n",
    "      model = MulticlassSVM_SMO(train_xs, train_ys, width, num_classes = 10, C = C)\n",
    "\n",
    "      # Train model\n",
    "      model.train()\n",
    "\n",
    "      # Evaluate on training and validation sets\n",
    "      train_accuracy = model.test(fold_train_xs, fold_train_ys)\n",
    "      val_accuracy = model.test(fold_validation_xs, fold_validation_ys)\n",
    "\n",
    "      train_accuracy_list[fold] = train_accuracy\n",
    "      val_accuracy_list[fold] = val_accuracy\n",
    "\n",
    "    print(f\"C: {C}. Gaussian kernel width: {width}. Run: {run}. Train accuracy list: {train_accuracy_list}. Val accuracy: {val_accuracy_list}.\")\n",
    "\n",
    "    mean_val_accuracy_list[i] = np.mean(val_accuracy_list)\n",
    "\n",
    "  # Get the width with maximum mean validation accuracy\n",
    "  argmax = np.argmax(mean_val_accuracy_list)\n",
    "  (C_star, width_star) = HYPERPARAMETERS[argmax]\n",
    "  \n",
    "  # Initialise model\n",
    "  model = MulticlassSVM_SMO(train_xs, train_ys, width_star, num_classes = 10, C = C_star)\n",
    "\n",
    "  # Train model\n",
    "  model.train()\n",
    "\n",
    "  # Evaluate on training and test sets\n",
    "  train_accuracy = model.test(train_xs, train_ys)\n",
    "  test_accuracy = model.test(test_xs, test_ys)\n",
    "\n",
    "  train_error = 1 - train_accuracy\n",
    "  test_error = 1 - test_accuracy\n",
    "\n",
    "  summary_data.append([C_star, width_star, train_error, test_error])\n",
    "\n",
    "# Create a DataFrame of the summary data\n",
    "summary = pd.DataFrame(summary_data, columns = ('C', 'width', 'train_error', 'test_error'))\n",
    "\n",
    "# Save summary as CSV\n",
    "csv_filename = f\"Q1_7a_smo_2_{datetime.now().strftime('%Y_%m_%d_%H_%M')}_{RUNS}_runs.csv\"\n",
    "summary.to_csv(f\"{path}/{csv_filename}\", index=False, float_format='%.3g')\n",
    "\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ToGELwrhRil"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "class MulticlassAdaBoost:\n",
    "  \"\"\"\n",
    "  Multi-class AdaBoost (SAMME) classifier with decision trees as weak learners,\n",
    "  using Gini impurity as the underlying metric\n",
    "  \"\"\"\n",
    "  def __init__(self, num_classes, num_classifiers, max_depth):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      num_classes: The number of classes\n",
    "      num_classifiers: The number of weak learners to train\n",
    "      max_depth: The maximum depth of the decision trees\n",
    "    \"\"\"\n",
    "\n",
    "    self.num_classes = num_classes\n",
    "    self.num_classifiers = num_classifiers\n",
    "    self.max_depth = max_depth\n",
    "\n",
    "    # Initialised by train method\n",
    "    self.classifiers = None\n",
    "    self.learner_weights = None\n",
    "  \n",
    "  def train(self, train_xs, train_ys):\n",
    "\n",
    "    num_examples, num_features = train_xs.shape\n",
    "\n",
    "    # Initialise weights to 1/n\n",
    "    example_weights = np.repeat(1/num_examples, num_examples)\n",
    "\n",
    "    # Initialise weak learners and their weights\n",
    "    self.classifiers = []\n",
    "    self.learner_weights = np.empty((self.num_classifiers))\n",
    "\n",
    "    for m in range(self.num_classifiers):\n",
    "\n",
    "      # Train learner and predict on training set\n",
    "      classifier = DecisionTreeClassifier(max_depth=self.max_depth)\n",
    "      classifier.fit(train_xs, train_ys, sample_weight = example_weights)\n",
    "      predictions = classifier.predict(train_xs)\n",
    "\n",
    "      mistakes = predictions != train_ys\n",
    "\n",
    "      # Compute the weighted error\n",
    "      weighted_error = example_weights.dot(mistakes)\n",
    "\n",
    "      # Compute the learner weight\n",
    "      learner_weight = np.log(1 - weighted_error) - np.log(weighted_error) + np.log(self.num_classes - 1)\n",
    "\n",
    "      # Scale and normalise weights\n",
    "      example_weights *= np.exp(np.dot(learner_weight, mistakes))\n",
    "      example_weights /= np.sum(example_weights)\n",
    "\n",
    "      self.classifiers.append(classifier)\n",
    "      self.learner_weights[m] = learner_weight\n",
    "\n",
    "    return self\n",
    "\n",
    "  def test(self, test_xs, test_ys):\n",
    "\n",
    "      num_examples = test_xs.shape[0]\n",
    "      labels = np.arange(self.num_classes)[:, np.newaxis]\n",
    "\n",
    "      projections = np.zeros((num_examples, self.num_classes))\n",
    "\n",
    "      for classifier, learner_weight in zip(self.classifiers, self.learner_weights):\n",
    "        projections += np.dot(learner_weight, (classifier.predict(test_xs) == labels).T)\n",
    "\n",
    "      predictions = np.argmax(projections, axis=1)\n",
    "\n",
    "      # Compute test accuracy\n",
    "      correct = predictions == test_ys\n",
    "      test_accuracy = sum(correct) / num_examples\n",
    "\n",
    "      return test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rv4Zow0IwA8P"
   },
   "outputs": [],
   "source": [
    "# 1. Basic results\n",
    "\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "TRAIN_TEST_SPLIT = 0.8\n",
    "\n",
    "NUM_CLASSIFIERS = [25, 50, 100]\n",
    "MAX_DEPTHS = [1, 2, 4, 8]\n",
    "\n",
    "HYPERPARAMETERS = [(num_classifiers, max_depth) for num_classifiers in NUM_CLASSIFIERS for max_depth in MAX_DEPTHS]\n",
    "\n",
    "RUNS = 20\n",
    "\n",
    "summary_data = []\n",
    "\n",
    "for (num_classifiers, max_depth) in HYPERPARAMETERS:\n",
    "\n",
    "  train_accuracy_list = np.empty((RUNS))\n",
    "  test_accuracy_list = np.empty((RUNS))\n",
    "\n",
    "  for run in range(RUNS):\n",
    "\n",
    "    # Generate train/test split\n",
    "    train_xs, train_ys, test_xs, test_ys = split_data(data_xs, data_ys, frac=TRAIN_TEST_SPLIT)\n",
    "\n",
    "    # Initialise model\n",
    "    model = MulticlassAdaBoost(NUM_CLASSES, num_classifiers, max_depth)\n",
    "\n",
    "    # Train model\n",
    "    model.train(train_xs, train_ys)\n",
    "\n",
    "    # Evaluate on training and test sets\n",
    "    train_accuracy = model.test(train_xs, train_ys)\n",
    "    test_accuracy = model.test(test_xs, test_ys)\n",
    "\n",
    "    train_accuracy_list[run] = train_accuracy\n",
    "    test_accuracy_list[run] = test_accuracy\n",
    "\n",
    "    print(f\"Number of classifiers: {num_classifiers}. Max depth: {max_depth}. Train accuracy: {train_accuracy}. Test accuracy: {test_accuracy}.\")\n",
    "\n",
    "  # Calculate mean and std for train and test accuracy\n",
    "  train_error_mean, train_error_std = 1 - np.mean(train_accuracy_list), np.std(train_accuracy_list)\n",
    "  test_error_mean, test_error_std = 1 - np.mean(test_accuracy_list), np.std(test_accuracy_list)\n",
    "\n",
    "  summary_data.append([num_classifiers, max_depth, train_error_mean, train_error_std, test_error_mean, test_error_std])\n",
    "  \n",
    "# Create a DataFrame of the summary data\n",
    "summary = pd.DataFrame(summary_data, columns = ('num_classifiers', 'max_depth', 'train_error_mean', 'train_error_std', 'test_error_mean', 'test_error_std'))\n",
    "\n",
    "# Save summary as CSV\n",
    "csv_filename = f\"Q1_7b_1_{datetime.now().strftime('%Y_%m_%d_%H_%M')}.csv\"\n",
    "summary.to_csv(f\"{path}/{csv_filename}\", index=False, float_format='%.3g')\n",
    "\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4IW6VuHtULrb"
   },
   "outputs": [],
   "source": [
    "# 2. Cross-validation\n",
    "\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "TRAIN_TEST_SPLIT = 0.8\n",
    "\n",
    "NUM_CLASSIFIERS = [25, 50, 100]\n",
    "MAX_DEPTHS = [1, 2, 4, 8]\n",
    "\n",
    "HYPERPARAMETERS = [(num_classifiers, max_depth) for num_classifiers in NUM_CLASSIFIERS for max_depth in MAX_DEPTHS]\n",
    "\n",
    "RUNS = 20\n",
    "\n",
    "FOLDS = 5\n",
    "\n",
    "summary_data = []\n",
    "\n",
    "for run in range(RUNS):\n",
    "\n",
    "  mean_val_accuracy_list = np.empty((len(HYPERPARAMETERS)))\n",
    "\n",
    "  # Generate train/test split\n",
    "  train_xs, train_ys, test_xs, test_ys = split_data(data_xs, data_ys, frac=TRAIN_TEST_SPLIT)\n",
    "\n",
    "  for i, (num_classifiers, max_depth) in enumerate(HYPERPARAMETERS):\n",
    "\n",
    "    # Generate validation folds\n",
    "    validation_folds = generate_validation_folds(train_xs, train_ys, k=5)\n",
    "\n",
    "    train_accuracy_list = np.empty((FOLDS))\n",
    "    val_accuracy_list = np.empty((FOLDS))\n",
    "\n",
    "    for fold, (fold_train_xs, fold_train_ys, fold_validation_xs, fold_validation_ys) in enumerate(validation_folds):\n",
    "\n",
    "      # Initialise model\n",
    "      model = MulticlassAdaBoost(NUM_CLASSES, num_classifiers, max_depth)\n",
    "\n",
    "      # Train model\n",
    "      model.train(fold_train_xs, fold_train_ys)\n",
    "\n",
    "      # Evaluate on training and validation sets\n",
    "      train_accuracy = model.test(fold_train_xs, fold_train_ys)\n",
    "      val_accuracy = model.test(fold_validation_xs, fold_validation_ys)\n",
    "\n",
    "      train_accuracy_list[fold] = train_accuracy\n",
    "      val_accuracy_list[fold] = val_accuracy\n",
    "\n",
    "    print(f\"Number of classifiers: {num_classifiers}. Max depth: {max_depth}. Run: {run}. Train accuracy list: {train_accuracy_list}. Val accuracy: {val_accuracy_list}.\")\n",
    "\n",
    "    mean_val_accuracy_list[i] = np.mean(val_accuracy_list)\n",
    "\n",
    "  # Get the width with maximum mean validation accuracy\n",
    "  argmax = np.argmax(mean_val_accuracy_list)\n",
    "  (num_classifiers_star, max_depth_star) = HYPERPARAMETERS[argmax]\n",
    "  \n",
    "  # Initialise model\n",
    "  model = MulticlassAdaBoost(NUM_CLASSES, num_classifiers_star, max_depth_star)\n",
    "\n",
    "  # Train model\n",
    "  model.train(train_xs, train_ys)\n",
    "\n",
    "  # Evaluate on training and test sets\n",
    "  train_accuracy = model.test(train_xs, train_ys)\n",
    "  test_accuracy = model.test(test_xs, test_ys)\n",
    "\n",
    "  train_error = 1 - train_accuracy\n",
    "  test_error = 1 - test_accuracy\n",
    "\n",
    "  summary_data.append([num_classifiers_star, max_depth_star, train_error, test_error])\n",
    "\n",
    "# Create a DataFrame of the summary data\n",
    "summary = pd.DataFrame(summary_data, columns = ('num_classifiers', 'max_depth', 'train_error', 'test_error'))\n",
    "\n",
    "# Save summary as CSV\n",
    "csv_filename = f\"Q1_7b_2_{datetime.now().strftime('%Y_%m_%d_%H_%M')}_{RUNS}_runs.csv\"\n",
    "summary.to_csv(f\"{path}/{csv_filename}\", index=False, float_format='%.3g')\n",
    "\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y1ibzQiUT6WW"
   },
   "source": [
    "# Part II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l-5pMXklkGH3"
   },
   "source": [
    "## 1. Sparse learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0HA3ZaXZkLwP"
   },
   "source": [
    "### Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pN0nCSyRErbd"
   },
   "outputs": [],
   "source": [
    "def generate_sample(m, n, algorithm=None):\n",
    "  \"\"\"\n",
    "  Generate m patterns uniformly at random from {-1, 1}^n,\n",
    "  or {0, 1}^n if the algorithm is winnow\n",
    "  \"\"\"\n",
    "  if algorithm == winnow:\n",
    "    X = np.random.choice([0, 1], (m, n))\n",
    "    y = X[:,0]\n",
    "  else:\n",
    "    X = np.random.choice([-1, 1], (m, n))\n",
    "    y = X[:,0]\n",
    "  return X, y\n",
    "\n",
    "def perceptron(X_train, y_train, X_test):\n",
    "  \"\"\"\n",
    "  Train a perceptron on train patterns X_train and labels y_train,\n",
    "  and evaluate on a test dataset X_test\n",
    "  \"\"\"\n",
    "  weights = np.zeros((X_train.shape[1]))\n",
    "\n",
    "  mistakes = 0\n",
    "\n",
    "  # Training loop\n",
    "  for i in range(X_train.shape[0]):\n",
    "    x_i, y_i = X_train[i], y_train[i]\n",
    "\n",
    "    y_i_hat = np.sign(weights @ x_i)\n",
    "\n",
    "    # Update\n",
    "    if (y_i_hat*y_i <= 0):\n",
    "      weights += y_i*x_i\n",
    "      mistakes += 1\n",
    "\n",
    "  # Test\n",
    "  return np.sign(X_test @ weights)\n",
    "\n",
    "def winnow(X_train, y_train, X_test):\n",
    "  \"\"\"\n",
    "  Train a winnow classifier on train patterns X_train and labels y_train,\n",
    "  and evaluate on a test dataset X_test\n",
    "  \"\"\"\n",
    "  n = X_train.shape[1]\n",
    "\n",
    "  weights = np.ones((X_train.shape[1]))\n",
    "\n",
    "  mistakes = 0\n",
    "\n",
    "  # Training loop\n",
    "  for i in range(X_train.shape[0]):\n",
    "    x_i, y_i = X_train[i], y_train[i]\n",
    "\n",
    "    y_i_hat = 0 if weights @ x_i < n else 1\n",
    "\n",
    "    # Update\n",
    "    if (y_i_hat != y_i):\n",
    "      weights *= np.float_power(2, ((y_i - y_i_hat) * x_i))\n",
    "      mistakes += 1\n",
    "\n",
    "  # Test\n",
    "  return np.where(X_test @ weights < n, 0, 1)\n",
    "\n",
    "def least_squares(X_train, y_train, X_test):\n",
    "  \"\"\"\n",
    "  Train a least squares classifier on train patterns X_train and labels y_train,\n",
    "  and evaluate on a test dataset X_test\n",
    "  \"\"\"\n",
    "  # Training\n",
    "  weights = np.linalg.pinv(X_train) @ y_train\n",
    "\n",
    "  # Test\n",
    "  return np.sign(X_test @ weights)\n",
    "\n",
    "def one_nearest_neighbour(X_train, y_train, X_test):\n",
    "  \"\"\"\n",
    "  Given a dataset of train patterns X_train and labels y_train, evaluate a\n",
    "  one-nearest-neighbour classifier on a test dataset X_test\n",
    "  \"\"\"\n",
    "  X_train = np.expand_dims(X_train, 2)\n",
    "  X_test = np.swapaxes(np.expand_dims(X_test, 2), 0, 2)\n",
    "\n",
    "  # Test\n",
    "  nearest = np.argmin(np.count_nonzero(X_train != X_test, axis = 1), axis = 0)\n",
    "  return y_train[nearest]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a8y6fPouE6eE"
   },
   "source": [
    "### 1.a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LKGPMFKuWSZa"
   },
   "outputs": [],
   "source": [
    "def estimate_generalization_error(m, n, algorithm, runs):\n",
    "\n",
    "  # Collect the test error for each run\n",
    "  test_errors = np.empty((runs))\n",
    "\n",
    "  for run in range(runs):\n",
    "\n",
    "    # Generate a train and test sample\n",
    "    X_train, y_train = generate_sample(m, n, algorithm)\n",
    "    X_test, y_test = generate_sample(TEST_SIZE, n, algorithm)\n",
    "\n",
    "    # Train algorithm and predict on the test sample\n",
    "    predictions = algorithm(X_train, y_train, X_test)\n",
    "    mistakes = np.count_nonzero(predictions != y_test)\n",
    "\n",
    "    # Calculate the test error\n",
    "    test_error = mistakes / TEST_SIZE\n",
    "\n",
    "    test_errors[run] = test_error\n",
    "  \n",
    "  # Estimate the generalization error by taking the mean over the runs\n",
    "  generalization_error = np.mean(test_errors)\n",
    "\n",
    "  return generalization_error\n",
    "\n",
    "def sample_complexity(algorithm, dimensions, runs):\n",
    "  \"\"\"\n",
    "  Estimate the number of samples (m) to obtain 10% generalization error versus\n",
    "  each dimension (n). Loop for m = 1, incrementing m, until the estimated\n",
    "  generalization error is less than 0.1, for each dimension n.\n",
    "  \"\"\"\n",
    "\n",
    "  # Collect estimated number of samples needed for each dimension\n",
    "  estimated_samples_by_dimension = np.empty((len(dimensions)))\n",
    "\n",
    "  for dimension_index, n in enumerate(dimensions):\n",
    "\n",
    "      # Initialise\n",
    "      m = 1\n",
    "      generalization_error = float('inf')\n",
    "\n",
    "      # Loop until estimated generalization error <= 0.1\n",
    "      while m < MAX_M:\n",
    "        \n",
    "        generalization_error = estimate_generalization_error(m, n, algorithm, runs)\n",
    "\n",
    "        if generalization_error <= 0.1:\n",
    "          print(f\"Dimension: {n}. Estimated m: {m}.\")\n",
    "          estimated_samples_by_dimension[dimension_index] = m\n",
    "          break\n",
    "\n",
    "        m += 1\n",
    "  \n",
    "  return estimated_samples_by_dimension\n",
    "\n",
    "def sample_complexity_binary_search(algorithm, dimensions, runs, lower, upper):\n",
    "  \"\"\"\n",
    "  Estimate the number of samples (m) to obtain 10% generalization error versus\n",
    "  each dimension (n). Perform binary search between L and R to find the smallest\n",
    "  such m for each n.\n",
    "  \"\"\"\n",
    "\n",
    "  # Collect estimated number of samples needed for each dimension\n",
    "  estimated_samples_by_dimension = np.empty((len(dimensions)))\n",
    "\n",
    "  for dimension_index, n in enumerate(dimensions):\n",
    "\n",
    "    L = lower\n",
    "    R = upper\n",
    "\n",
    "    # Initialise\n",
    "    best_m = float('inf')\n",
    "\n",
    "    # Loop until L == R\n",
    "    while L <= R:\n",
    "\n",
    "      if L == R or L + 1 == R:\n",
    "        L_error = estimate_generalization_error(L, n, algorithm, runs)\n",
    "        best_m = L if L_error < 0.1 else R\n",
    "        estimated_samples_by_dimension[dimension_index] = best_m\n",
    "        print(f\"Dimension: {n}. Estimated m: {best_m}.\")\n",
    "        break\n",
    "\n",
    "      m = (L + R) // 2\n",
    "\n",
    "      generalization_error = estimate_generalization_error(m, n, algorithm, runs)\n",
    "\n",
    "      if generalization_error < 0.1:\n",
    "        R = m\n",
    "\n",
    "        if m < best_m:\n",
    "          best_m = m\n",
    "          best_generalization_error = generalization_error\n",
    "          print('Current best:', best_m, best_generalization_error)\n",
    "      \n",
    "      if generalization_error > 0.1:\n",
    "        L = m\n",
    "  \n",
    "  return estimated_samples_by_dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nq2T579GLufT"
   },
   "outputs": [],
   "source": [
    "def plot_sample_complexity(algorithm, dimensions, samples_by_dimension_min):\n",
    "  plt.plot(dimensions, samples_by_dimension_min)\n",
    "  plt.xlabel('n')\n",
    "  plt.ylabel('m')\n",
    "  plt.title(f\"Estimated number of samples (m) to obtain 10%\\ngeneralisation error versus dimension (n) for {algorithm.__name__}\")\n",
    "  plt.savefig(f\"Q2_a_{algorithm.__name__}\", dpi=300)\n",
    "  plt.show()\n",
    "\n",
    "def plot_linear_fit(algorithm, dimensions, estimated_samples_by_dimension):\n",
    "  a, b = np.polyfit(dimensions, estimated_samples_by_dimension, 1)\n",
    "\n",
    "  # Print equation\n",
    "  a_round, b_round = round(a, 3), round(b, 3)\n",
    "  print(f\"Algorithm: {algorithm.__name__}. Equation: {b_round} + {a_round}n.\")\n",
    "\n",
    "  # Plot estimate and fit\n",
    "  plt.plot(dimensions, estimated_samples_by_dimension, label='Estimate')\n",
    "  plt.plot(dimensions, a*dimensions + b, label='Fit')\n",
    "  plt.xlabel('n')\n",
    "  plt.ylabel('m')\n",
    "  plt.legend()\n",
    "  plt.title(f\"Linear fit of number of samples (m) to obtain 10%\\n generalisation error versus dimension (n) for {algorithm.__name__}\")\n",
    "  plt.savefig(f\"Q2_a_{algorithm.__name__}_linear_fit\", dpi=300)\n",
    "  plt.show()\n",
    "\n",
    "def plot_log_fit(algorithm, dimensions, estimated_samples_by_dimension):\n",
    "  a, b = np.polyfit(np.log(dimensions), estimated_samples_by_dimension, 1)\n",
    "\n",
    "  # Print equation\n",
    "  a_round, b_round = round(a, 3), round(b, 3)\n",
    "  print(f\"Algorithm: {algorithm.__name__}. Equation: {b_round} + {a_round}log(n).\")\n",
    "\n",
    "  # Plot estimate and fit\n",
    "  plt.plot(dimensions, estimated_samples_by_dimension, label='Estimate')\n",
    "  plt.plot(dimensions, a*np.log(dimensions) + b, label='Fit')\n",
    "  plt.xlabel('n')\n",
    "  plt.ylabel('m')\n",
    "  plt.legend()\n",
    "  plt.title(f\"Log fit of number of samples (m) to obtain 10%\\ngeneralisation error versus dimension (n) for {algorithm.__name__}\")\n",
    "  plt.savefig(f\"Q2_a_{algorithm.__name__}_log_fit\", dpi=300)\n",
    "  plt.show()\n",
    "\n",
    "def plot_exponential_fit(algorithm, dimensions, estimated_samples_by_dimension):\n",
    "  a, b = np.polyfit(dimensions, np.log(estimated_samples_by_dimension), 1)\n",
    "  \n",
    "  # Print equation\n",
    "  a_round, b_round = round(a, 3), round(b, 3)\n",
    "  print(f\"Algorithm: {algorithm.__name__}. Equation: {b_round}e^({a_round})n.\")\n",
    "\n",
    "  # Plot estimate and fit\n",
    "  plt.plot(dimensions, estimated_samples_by_dimension, label='Estimate')\n",
    "  plt.plot(dimensions, np.exp(a*dimensions + b), label='Fit')\n",
    "  plt.xlabel('n')\n",
    "  plt.ylabel('m')\n",
    "  plt.legend()\n",
    "  plt.title(f\"Exponential fit of number of samples (m) to obtain 10%\\ngeneralisation error versus dimension (n) for {algorithm.__name__}\")\n",
    "  plt.savefig(f\"Q2_a_{algorithm.__name__}_exponential_fit\", dpi=300)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oQJrRER0QpVA"
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "RUNS = 5\n",
    "TEST_SIZE = 10000\n",
    "MAX_M = 10000 # Max sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hVljbJm4QgDM"
   },
   "outputs": [],
   "source": [
    "# Perceptron\n",
    "dimensions = range(1, 101)\n",
    "\n",
    "estimated_samples_by_dimension = sample_complexity(perceptron, dimensions, RUNS)\n",
    "\n",
    "plot_sample_complexity(perceptron, dimensions, estimated_samples_by_dimension)\n",
    "plot_linear_fit(perceptron, dimensions, estimated_samples_by_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "47kjT2GCQhmt"
   },
   "outputs": [],
   "source": [
    "# Winnow\n",
    "dimensions = range(1, 101)\n",
    "\n",
    "estimated_samples_by_dimension = sample_complexity(winnow, dimensions, RUNS)\n",
    "\n",
    "plot_sample_complexity(winnow, dimensions, estimated_samples_by_dimension)\n",
    "plot_log_fit(winnow, dimensions, estimated_samples_by_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VQDL32M9Qis7"
   },
   "outputs": [],
   "source": [
    "# Least squares\n",
    "dimensions = range(100, 101)\n",
    "\n",
    "estimated_samples_by_dimension = sample_complexity(least_squares, dimensions, RUNS)\n",
    "\n",
    "plot_sample_complexity(least_squares, dimensions, estimated_samples_by_dimension)\n",
    "plot_linear_fit(least_squares, dimensions, estimated_samples_by_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FdJ1-mMYQi5V"
   },
   "outputs": [],
   "source": [
    "# 1-nearest neighbours\n",
    "dimensions = range(1, 21)\n",
    "\n",
    "estimated_samples_by_dimension = sample_complexity(one_nearest_neighbour, dimensions, RUNS)\n",
    "\n",
    "plot_sample_complexity(one_nearest_neighbour, dimensions, estimated_samples_by_dimension)\n",
    "plot_exponential_fit(one_nearest_neighbour, dimensions, estimated_samples_by_dimension)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "COMP0078: Coursework 2",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
