{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Coursework 1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cdetl-_ZNB8K"
      },
      "source": [
        "# Import libs\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import pandas as pd\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDHRb8OzhjfD"
      },
      "source": [
        "# Part I"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdB4IfaBhqEt"
      },
      "source": [
        "## 1.1. Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9CLhB3Ph_NT"
      },
      "source": [
        "### Exercise 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9_lwOCiNbqc"
      },
      "source": [
        "def polynomial_matrix(x, degree):\n",
        "  \"\"\"Compute the polynomial basis matrix of the given degree for vector x\"\"\"\n",
        "  result = np.empty((x.shape[0], degree))\n",
        "  for k in range(0, degree): # Adds bias\n",
        "    result[:,k] = x**k\n",
        "  return result\n",
        "\n",
        "def fit_polynomial(x, y, degree):\n",
        "  \"\"\"Fit a polynomial basis of the given degree to a data set\"\"\"\n",
        "  X = polynomial_matrix(x, degree)\n",
        "  weights = np.linalg.pinv(X) @ y\n",
        "  return weights;\n",
        "\n",
        "# Data set\n",
        "x = np.array([1,2,3,4])\n",
        "y = np.array([3,2,0,5])\n",
        "\n",
        "# 1a\n",
        "for k in range(1, 5):\n",
        "  # Fit a polynomial of dimension k to the data set\n",
        "  weights = fit_polynomial(x, y, k)\n",
        "  curve_x = np.arange(0,5,0.01)\n",
        "  curve_y = np.matmul(polynomial_matrix(curve_x, k), weights)\n",
        "  # Plot the fitted polynomial\n",
        "  plt.plot(curve_x, curve_y, label=f'k={k}')\n",
        "\n",
        "# Plot the data set\n",
        "plt.scatter(x, y)\n",
        "\n",
        "plt.ylim((-4, 8))\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.legend()\n",
        "plt.savefig(f\"q1a\", dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# 1b\n",
        "for k in range(1, 5):\n",
        "  # Print the weights for each fitted polynomial\n",
        "  weights = fit_polynomial(x, y, k)\n",
        "  print(f'The weights for k={k} are: {weights}')\n",
        "\n",
        "# 1c\n",
        "for k in range(1, 5):\n",
        "  # Compute MSE on data set\n",
        "  weights = fit_polynomial(x, y, k)\n",
        "  X = polynomial_matrix(x, k)\n",
        "  sse = np.sum((np.matmul(X, weights) - y)**2)\n",
        "  mse = sse/4\n",
        "  print(f'The MSE for k={k} is: {mse}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J48W1BbniDrD"
      },
      "source": [
        "### Exercise 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sOfksAsWNTM"
      },
      "source": [
        "# 2a i\n",
        "def f(x):\n",
        "  return np.sin(2*np.pi*x)**2\n",
        "\n",
        "def g(sigma, x):\n",
        "  return f(x) + np.random.normal(0, sigma, x.shape[0])\n",
        "\n",
        "# Generate training set\n",
        "size_of_training_set = 30\n",
        "\n",
        "training_x = np.random.random(size_of_training_set)\n",
        "training_y = g(0.07, training_x)\n",
        "\n",
        "# Plot the function without random noise\n",
        "curve_x = np.arange(0,1,0.01)\n",
        "curve_y = f(curve_x)\n",
        "\n",
        "plt.scatter(training_x, training_y)\n",
        "plt.plot(curve_x, curve_y)\n",
        "\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.savefig(f\"q2ai\", dpi=300)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PMelVm2v7qI"
      },
      "source": [
        "# 2a ii\n",
        "curve_x = np.arange(0,1,0.01)\n",
        "\n",
        "for k in (2, 5, 10, 14, 18):\n",
        "  curve_y = np.matmul(polynomial_matrix(curve_x, k), fit_polynomial(training_x, training_y, k))\n",
        "  plt.plot(curve_x, curve_y, label=f\"k={k}\", c='orange')\n",
        "\n",
        "  plt.scatter(training_x, training_y)\n",
        "\n",
        "  plt.ylim((0, 1.3))\n",
        "\n",
        "  plt.xlabel('x')\n",
        "  plt.ylabel('y')\n",
        "  plt.legend()\n",
        "  plt.savefig(f\"q2aii_{k}\", dpi=300)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtW1e1OXYdzp"
      },
      "source": [
        "# 2b\n",
        "dimensions = range(1, 18+1)\n",
        "mean_squared_errors = []\n",
        "\n",
        "for k in dimensions:\n",
        "  # Compute MSE on training set\n",
        "  weights = fit_polynomial(training_x, training_y, k)\n",
        "  X = polynomial_matrix(training_x, k)\n",
        "  sse = np.sum((training_y - np.matmul(X, weights)**2))\n",
        "  mean_squared_errors.append(sse/size_of_training_set)\n",
        "\n",
        "plt.plot(dimensions, np.log(mean_squared_errors), label='Training set')\n",
        "plt.xlabel('Polynomial dimension')\n",
        "plt.xticks(np.arange(0, 19, step = 2), np.arange(0, 19, step = 2))\n",
        "plt.ylabel('Mean squared error (ln)')\n",
        "plt.legend()\n",
        "plt.savefig(f\"q2b\", dpi=300)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fraJy4YtEyMR"
      },
      "source": [
        "# 2c\n",
        "# Generate test set\n",
        "size_of_test_set = 1000\n",
        "\n",
        "test_x = np.random.random(size_of_test_set)\n",
        "test_y = g(0.07, test_x)\n",
        "\n",
        "dimensions = range(1, 18+1)\n",
        "mean_squared_errors = []\n",
        "\n",
        "for k in dimensions:\n",
        "  # Compute MSE on test set\n",
        "  weights = fit_polynomial(training_x, training_y, k)\n",
        "  X = polynomial_matrix(test_x, k)\n",
        "  sse = np.sum((test_y - np.matmul(X, weights))**2)\n",
        "  mean_squared_errors.append(sse/size_of_test_set)\n",
        "\n",
        "plt.plot(dimensions, np.log(mean_squared_errors), label='Test set')\n",
        "plt.xlabel('Polynomial dimension')\n",
        "plt.xticks(np.arange(0, 19, step = 2), np.arange(0, 19, step = 2))\n",
        "plt.ylabel('Mean squared error (ln)')\n",
        "plt.legend()\n",
        "plt.savefig(f\"q2c\", dpi=300)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMKSSvAoGcGz"
      },
      "source": [
        "# 2d\n",
        "# Repeat (b) and (c) for 100 runs\n",
        "number_of_runs = 100\n",
        "dimensions = range(1, 18+1)\n",
        "\n",
        "set_of_training_errors = []\n",
        "set_of_test_errors = []\n",
        "\n",
        "for i in range(number_of_runs):\n",
        "  # Generate training set\n",
        "  training_x = np.random.random(size_of_training_set)\n",
        "  training_y = g(0.07, training_x)\n",
        "\n",
        "  training_mean_squared_errors = []\n",
        "\n",
        "  for k in dimensions:\n",
        "    # Compute MSE on training set\n",
        "    weights = fit_polynomial(training_x, training_y, k)\n",
        "    X = polynomial_matrix(training_x, k)\n",
        "    sse = np.sum((training_y - np.matmul(X, weights))**2)\n",
        "    training_mean_squared_errors.append(sse/size_of_training_set)\n",
        "  \n",
        "  # Generate training set\n",
        "  test_x = np.random.random(size_of_test_set)\n",
        "  test_y = g(0.07, test_x)\n",
        "\n",
        "  test_mean_squared_errors = []\n",
        "\n",
        "  for k in dimensions:\n",
        "    # Compute MSE on test set\n",
        "    weights = fit_polynomial(training_x, training_y, k)\n",
        "    X = polynomial_matrix(test_x, k)\n",
        "    sse = np.sum((test_y - np.matmul(X, weights))**2)\n",
        "    test_mean_squared_errors.append(sse/size_of_test_set)\n",
        "\n",
        "  set_of_training_errors.append(training_mean_squared_errors)\n",
        "  set_of_test_errors.append(test_mean_squared_errors)\n",
        "\n",
        "set_of_training_errors = np.array(set_of_training_errors)\n",
        "set_of_test_errors = np.array(set_of_test_errors)\n",
        "\n",
        "# Average the training and test errors for each dimension k\n",
        "average_training_errors = set_of_training_errors.mean(axis=0)\n",
        "average_test_errors = set_of_test_errors.mean(axis=0)\n",
        "\n",
        "# Plot the natural log of the average training and test errors\n",
        "plt.plot(dimensions, np.log(average_training_errors), label='Training data')\n",
        "plt.plot(dimensions, np.log(average_test_errors), label='Test data')\n",
        "\n",
        "plt.xlabel('Polynomial dimension')\n",
        "plt.xticks(np.arange(0, 19, step = 2), np.arange(0, 19, step = 2))\n",
        "plt.ylabel('Average mean squared error (ln)')\n",
        "plt.legend()\n",
        "plt.savefig(f\"q2d\", dpi=300)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvlRhpKQiLAy"
      },
      "source": [
        "### Exercise 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ql9WvKJQ0km"
      },
      "source": [
        "def basis_matrix(x, degree):\n",
        "  \"\"\"Compute the basis matrix of the given degree for vector x\"\"\"\n",
        "  result = np.empty((x.shape[0], degree))\n",
        "  for k in range(1, degree+1):\n",
        "    result[:,k-1] = np.sin(k*np.pi*x)\n",
        "  return result\n",
        "\n",
        "def fit_basis(x, y, degree):\n",
        "  \"\"\"Fit a basis of the given degree to a data set\"\"\"\n",
        "  X = basis_matrix(x, degree)\n",
        "  weights = np.matmul(np.linalg.pinv(X), y)\n",
        "  return weights;\n",
        "\n",
        "dimensions = range(1, 18+1)\n",
        "size_of_training_set = 30\n",
        "size_of_test_set = 1000\n",
        "\n",
        "# Repeat of 2b\n",
        "mean_squared_errors = []\n",
        "\n",
        "# Generate training set\n",
        "training_x = np.random.random(size_of_training_set)\n",
        "training_y = g(0.07, training_x)\n",
        "\n",
        "for k in dimensions:\n",
        "  # Calculate MSE on training set\n",
        "  weights = fit_basis(training_x, training_y, k)\n",
        "  X = basis_matrix(training_x, k)\n",
        "  sse = np.sum((f(training_x) - np.matmul(X, weights))**2)\n",
        "  mean_squared_errors.append(sse/size_of_training_set)\n",
        "\n",
        "plt.plot(dimensions, np.log(mean_squared_errors), label='Training set')\n",
        "plt.xlabel('Basis dimension')\n",
        "plt.ylabel('Mean squared error (ln)')\n",
        "plt.legend()\n",
        "plt.savefig(f\"q3_2b\", dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# Repeat of 2c\n",
        "mean_squared_errors = []\n",
        "\n",
        "# Generate test set\n",
        "test_x = np.random.random(size_of_test_set)\n",
        "test_y = g(0.07, test_x)\n",
        "\n",
        "for k in dimensions:\n",
        "  # Compute MSE on test set\n",
        "  weights = fit_basis(training_x, training_y, k)\n",
        "  X = basis_matrix(test_x, k)\n",
        "  sse = np.sum((test_y - np.matmul(X, weights))**2)\n",
        "  mean_squared_errors.append(sse/size_of_test_set)\n",
        "\n",
        "plt.plot(dimensions, np.log(mean_squared_errors), label='Test set')\n",
        "plt.xlabel('Basis dimension')\n",
        "plt.ylabel('Mean squared error (ln)')\n",
        "plt.legend()\n",
        "plt.savefig(f\"q3_2c\", dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# Repeat of 2d\n",
        "number_of_runs = 100\n",
        "\n",
        "set_of_training_errors = []\n",
        "set_of_test_errors = []\n",
        "\n",
        "for i in range(number_of_runs):\n",
        "  # Generate training set\n",
        "  training_x = np.random.random(size_of_training_set)\n",
        "  training_y = g(0.07, training_x)\n",
        "\n",
        "  training_mean_squared_errors = []\n",
        "\n",
        "  for k in dimensions:\n",
        "    # Compute MSE on training set\n",
        "    weights = fit_basis(training_x, training_y, k)\n",
        "    X = basis_matrix(training_x, k)\n",
        "    sse = np.sum((training_y - np.matmul(X, weights))**2)\n",
        "    training_mean_squared_errors.append(sse/size_of_training_set)\n",
        "  \n",
        "  # Generate test set\n",
        "  test_x = np.random.random(size_of_test_set)\n",
        "  test_y = g(0.07, test_x)\n",
        "\n",
        "  test_mean_squared_errors = []\n",
        "\n",
        "  for k in dimensions:\n",
        "    # Compute MSE on test set\n",
        "    weights = fit_basis(training_x, training_y, k)\n",
        "    X = basis_matrix(test_x, k)\n",
        "    sse = np.sum((test_y - np.matmul(X, weights))**2)\n",
        "    test_mean_squared_errors.append(sse/size_of_test_set)\n",
        "\n",
        "  set_of_training_errors.append(training_mean_squared_errors)\n",
        "  set_of_test_errors.append(test_mean_squared_errors)\n",
        "\n",
        "set_of_training_errors = np.array(set_of_training_errors)\n",
        "set_of_test_errors = np.array(set_of_test_errors)\n",
        "\n",
        "# Average the training and test errors for each dimension k\n",
        "average_training_errors = set_of_training_errors.mean(axis=0)\n",
        "average_test_errors = set_of_test_errors.mean(axis=0)\n",
        "\n",
        "# Plot the natural log of the training and test errors\n",
        "plt.plot(dimensions, np.log(average_training_errors), label='Training data')\n",
        "plt.plot(dimensions, np.log(average_test_errors), label='Test data')\n",
        "\n",
        "plt.xlabel('Basis dimension')\n",
        "plt.ylabel('Average mean squared error (ln)')\n",
        "plt.legend()\n",
        "plt.savefig(f\"q3_2d\", dpi=300)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wq1VJShhaQQ"
      },
      "source": [
        "## 1.2: Filtered Boston housing and kernels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QakldFmWk1tR"
      },
      "source": [
        "# Import the data as 'housedata_df'\n",
        "housedata_df = pd.read_csv('http://www0.cs.ucl.ac.uk/staff/M.Herbster/boston-filter/Boston-filtered.csv')\n",
        "housedata_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvlCLTRgu6fk"
      },
      "source": [
        "housedata = housedata_df.to_numpy()\n",
        "\n",
        "def generate_samples(data, frac=2/3):\n",
        "  \"\"\"General function to generate training and test sets, as well as arrays of output values for each set\"\"\"\n",
        "  n = data.shape[0]\n",
        "  shuffled_indices = [*range(n)]\n",
        "  np.random.shuffle(shuffled_indices)\n",
        "  # Generate random training and test sets\n",
        "  training_n = int(frac * n)\n",
        "  training_indices, test_indices = shuffled_indices[:training_n], shuffled_indices[training_n:]\n",
        "  training_set, test_set = data[training_indices], data[test_indices]\n",
        "  # Extract relevant output variable (13th column) of training and test sets\n",
        "  training_price = training_set[:,12]\n",
        "  test_price = test_set[:,12]\n",
        "  return training_set, test_set, training_price, test_price"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kN3wDmUfiVLy"
      },
      "source": [
        "### Exercise 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKWNQKTWjKqs"
      },
      "source": [
        "#### (a) Naive regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3K8SMto4LRj"
      },
      "source": [
        "mse_training = []\n",
        "mse_test = []\n",
        "\n",
        "for i in range(20):\n",
        "  # Generate training and test sets, as well as arrays of output variable for each set\n",
        "  training_set_boston, test_set_boston, training_price_array, test_price_array = generate_samples(housedata)\n",
        "\n",
        "  # Create vectors of ones that are the same length as the training and testing sets, respectively\n",
        "  ones_training_set = np.ones(training_set_boston.shape[0])\n",
        "  ones_test_set = np.ones(test_set_boston.shape[0])\n",
        "\n",
        "  # Calculate single regressor parameter based on training set \n",
        "  regressor = (np.matmul(ones_training_set.T,ones_training_set))**(-1)*(np.matmul(ones_training_set.T,training_price_array))\n",
        "  \n",
        "  # Compute MSE on training set\n",
        "  mse_training_single_run = np.sum((training_price_array - (regressor*ones_training_set))**2)/training_set_boston.shape[0]\n",
        "  mse_training.append(mse_training_single_run)\n",
        "\n",
        "  # Compute MSE on test set\n",
        "  mse_test_single_run = np.sum((test_price_array - (regressor*ones_test_set))**2)/test_set_boston.shape[0]\n",
        "  mse_test.append(mse_test_single_run)\n",
        "\n",
        "# Print mean of MSE over 20 runs\n",
        "print(f'MSE training error over 20 runs based on naive regression: {round(sum(mse_training)/20, 2)}')\n",
        "print(f'MSE test error over 20 runs based on naive regression: {round(sum(mse_test)/20, 2)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBbeCP5dRWZL"
      },
      "source": [
        "We obtain the following result:\n",
        "\n",
        "*   MSE training error over 20 runs based on naive regression: 83.01\n",
        "*   MSE test error over 20 runs based on naive regression: 87.46"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_6Z7cbwin1R"
      },
      "source": [
        "#### (b) Interpretation of constant function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZaAQoSZMJ5S"
      },
      "source": [
        "The constant function is equivalent to the *mean value of column 13 for the training set*.\n",
        "\n",
        "This can be derived from the expression used to determine the regression parameter: the first factor ($X^TX$) is the inverse of the number of observations in the training set, whereas the second term ($X^TY$) is the sum of the values of column 13 (i.e. median house price) in the training set. In other words, the constant function is the mean value of the feature in question."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5yP_mJhi0IW"
      },
      "source": [
        "#### (c) Linear Regression with single attributes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeS722_eFDfP"
      },
      "source": [
        "for k in range(0,12):\n",
        "\n",
        "  mse_training = []\n",
        "  mse_test = []\n",
        "  for i in range(20):\n",
        "  # Generate training and test sets, as well as arrays of output variable for each set\n",
        "    training_set_boston, test_set_boston, training_price_array, test_price_array = generate_samples(housedata)\n",
        "\n",
        "  # Extract relevant input variable of training and test sets, and generate arrays\n",
        "    training_feature_array = training_set_boston[:,k]\n",
        "    test_feature_array = test_set_boston[:,k]\n",
        "  \n",
        "  # Add bias\n",
        "    ones_training_set = np.ones(training_set_boston.shape[0])\n",
        "    ones_test_set = np.ones(test_set_boston.shape[0])\n",
        "    training_matrix_single = np.stack((training_feature_array, ones_training_set), axis = 1)\n",
        "    test_matrix_single = np.stack((test_feature_array, ones_test_set), axis = 1)\n",
        "\n",
        "  # Calculate regressor vector based on training set \n",
        "    regressor_single = np.matmul((np.linalg.inv(np.matmul(training_matrix_single.T,training_matrix_single))),(np.matmul(training_matrix_single.T,training_price_array)))\n",
        "\n",
        "  # Compute MSE on training set\n",
        "    mse_training_single_run = np.sum((training_price_array - (np.matmul(training_matrix_single, regressor_single)))**2)/training_set_boston.shape[0]\n",
        "    mse_training.append(mse_training_single_run)\n",
        "\n",
        "  # Compute MSE on test set\n",
        "    mse_test_single_run = np.sum((test_price_array - (np.matmul(test_matrix_single, regressor_single)))**2)/test_set_boston.shape[0]\n",
        "    mse_test.append(mse_test_single_run)\n",
        "\n",
        "  print(f'Attribute {k+1} ({housedata_df.columns[k].strip()})')\n",
        "  print(f'Training MSE over 20 runs: {round(sum(mse_training)/20, 2)}')\n",
        "  print(f'Test MSE over 20 runs: {round(sum(mse_test)/20, 2)}')\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fr8obBt-Rq1I"
      },
      "source": [
        "We obtain the following results:\n",
        "\n",
        "1.   Attribute 1 (CRIM)\n",
        "\n",
        "  *   Training MSE over 20 runs: 69.30\n",
        "  *   Test MSE over 20 runs: 77.31\n",
        "\n",
        "2.   Attribute 2 (ZN)\n",
        "\n",
        "  *   Training MSE over 20 runs: 71.50\n",
        "  *   Test MSE over 20 runs: 77.79\n",
        "\n",
        "3.   Attribute 3 (INDUS)\n",
        "\n",
        "  *   Training MSE over 20 runs: 65.96\n",
        "  *   Test MSE over 20 runs: 62.47\n",
        "\n",
        "4.   Attribute 4 (CHAS)\n",
        "  *   Training MSE over 20 runs: 81.06\n",
        "  *   Test MSE over 20 runs: 83.75\n",
        "\n",
        "5.   Attribute 5 (NOX)\n",
        "  *   Training MSE over 20 runs: 70.83\n",
        "  *   Test MSE over 20 runs: 65.61\n",
        "\n",
        "6.   Attribute 6 (RM)\n",
        "  *   Training MSE over 20 runs: 44.81\n",
        "  *   Test MSE over 20 runs: 41.55\n",
        "\n",
        "7.   Attribute 7 (AGE)\n",
        "  *   Training MSE over 20 runs: 69.09\n",
        "  *   Test MSE over 20 runs: 79.59\n",
        "\n",
        "8.   Attribute 8 (DIS)\n",
        "  *   Training MSE over 20 runs: 80.00\n",
        "  *   Test MSE over 20 runs: 77.85\n",
        "\n",
        "9.   Attribute 9 (RAD)\n",
        "  *   Training MSE over 20 runs: 73.66\n",
        "  *   Test MSE over 20 runs: 69.44\n",
        "\n",
        "10.   Attribute 10 (TAX)\n",
        "  *   Training MSE over 20 runs: 66.08\n",
        "  *   Test MSE over 20 runs: 65.86\n",
        "\n",
        "11.   Attribute 11 (PTRATIO)\n",
        "  *   Training MSE over 20 runs: 63.38\n",
        "  *   Test MSE over 20 runs: 61.65\n",
        "\n",
        "12.   Attribute 12 (LSTAT)\n",
        "  *   Training MSE over 20 runs: 38.62\n",
        "  *   Test MSE over 20 runs: 38.59\n",
        "\n",
        "Taken in isolation, Attributes 6 and 12 provide the most accurate prediction for the value of the 13th column."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SE0IiiZ7i6MI"
      },
      "source": [
        "#### (d) Linear Regression using all attributes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-M24hROy8pg"
      },
      "source": [
        "  mse_training = []\n",
        "  mse_test = []\n",
        "\n",
        "  for i in range(20):\n",
        "    # Generate training and test sets, as well as arrays of output variable for each set\n",
        "    training_set_boston, test_set_boston, training_price_array, test_price_array = generate_samples(housedata)\n",
        "\n",
        "    # Extract relevant input variables of training and test sets, and generate arrays\n",
        "    training_features_all = training_set_boston[:,:-1]\n",
        "    test_features_all = test_set_boston[:,:-1]\n",
        "  \n",
        "    # Add bias\n",
        "    ones_training_set = np.ones((training_set_boston.shape[0], 1))\n",
        "    ones_test_set = np.ones((test_set_boston.shape[0], 1))\n",
        "    training_matrix_all = np.hstack((training_features_all, ones_training_set))\n",
        "    test_matrix_all = np.hstack((test_features_all, ones_test_set))\n",
        "\n",
        "    # Calculate regressor vector based on training set \n",
        "    regressor_all = np.matmul((np.linalg.inv(np.matmul(training_matrix_all.T,training_matrix_all))),(np.matmul(training_matrix_all.T,training_price_array)))\n",
        "\n",
        "    # Compute MSE on training set\n",
        "    mse_training_single_run = np.sum((training_price_array - (np.matmul(training_matrix_all, regressor_all)))**2)/training_set_boston.shape[0]\n",
        "    mse_training.append(mse_training_single_run)\n",
        "\n",
        "    # Compute MSE on test set\n",
        "    mse_test_single_run = np.sum((test_price_array - (np.matmul(test_matrix_all, regressor_all)))**2)/test_set_boston.shape[0]\n",
        "    mse_test.append(mse_test_single_run)\n",
        "\n",
        "  print(f'Training MSE over 20 runs: {round(sum(mse_training)/20, 2)}')\n",
        "  print(f'Test MSE over 20 runs: {round(sum(mse_test)/20, 2)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjgfMWb5Sthf"
      },
      "source": [
        "We obtain the following results:\n",
        "\n",
        "*   Training MSE over 20 runs: 21.98\n",
        "*   Test MSE over 20 runs: 24.99\n",
        "\n",
        "The MSEs for both the training and test sets are lower than for any of the models based on a single feature."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggxVZ_W7Ohpg"
      },
      "source": [
        "### Exercise 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foC-fKV0bIk-"
      },
      "source": [
        "\n",
        "\n",
        "#### (a) Cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJxwCfTrRfcB"
      },
      "source": [
        "# Generate training and test sets, as well as arrays of output variable for each set\n",
        "training_set_boston, test_set_boston, training_price_array, test_price_array = generate_samples(housedata)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjOGUHrRZigs"
      },
      "source": [
        "def gen_validation_folds(data, k):\n",
        "  \"\"\"Split data set into k parts. Returns a list of k lists of the forms\n",
        "  [training_set, validation_set], where the validation_set is a fraction (1/k)\n",
        "  of data, and the training_set is data without this chunk\"\"\"\n",
        "  result = []\n",
        "\n",
        "  n = data.shape[0]\n",
        "  shuffled_indices = np.arange(n)\n",
        "  np.random.shuffle(shuffled_indices)\n",
        "  # Split the shuffled indices into k chunks for the validation sets\n",
        "  for validation_indices in np.array_split(shuffled_indices, k):\n",
        "      training_indices = np.setdiff1d(shuffled_indices, validation_indices)\n",
        "      # Extract training and validation sets\n",
        "      training_set, validation_set = data[training_indices], data[validation_indices]\n",
        "      result.append([training_set, validation_set])\n",
        "  return result\n",
        "\n",
        "validation_folds = gen_validation_folds(training_set_boston, 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdIr3GqvNpZi"
      },
      "source": [
        "# Define kernel function\n",
        "def kernel(a, b, sigma):\n",
        "  \"\"\"Compute Gaussian kernel of a and b with parameter sigma\"\"\"\n",
        "  result = np.exp(-(np.linalg.norm(a-b))**2/(2*(sigma**2)))\n",
        "  return result\n",
        "\n",
        "# Create a vector of gamma values\n",
        "gamma_values = []\n",
        "gamma_powers = []\n",
        "for i in reversed(range(26, 40+1, 1)):\n",
        "  gamma_values.append(2**(-i))\n",
        "  gamma_powers.append(i)\n",
        "\n",
        "# Create a vector of sigma values\n",
        "sigma_values = []\n",
        "sigma_powers = []\n",
        "for i in np.arange(7, 13.5, 0.5):\n",
        "  sigma_values.append(2**i)\n",
        "  sigma_powers.append(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QR7ddcXOE9XM"
      },
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "error_matrix = np.zeros((len(gamma_values), len(sigma_values)))\n",
        "for g in range(len(gamma_values)):\n",
        "  gamma = gamma_values[g]\n",
        "  gamma_power = gamma_powers[g]\n",
        "  for s in range(len(sigma_values)):\n",
        "    sigma = sigma_values[s]\n",
        "    sigma_power = sigma_powers[s]\n",
        "    error = []\n",
        "    print(\"Computing: gamma_values[%s] --- sigma_values[%s] --- %s seconds\" % (g, s, (time.time() - start_time)))\n",
        "    for f in range(5):\n",
        "      training_fold = validation_folds[f][0]\n",
        "      validation_fold = validation_folds[f][1]\n",
        "\n",
        "      # Compute Kernel matrix\n",
        "      K_matrix = np.zeros((training_fold.shape[0], training_fold.shape[0]))\n",
        "      for i in range(training_fold.shape[0]):\n",
        "        for j in range(training_fold.shape[0]):\n",
        "          K_matrix[i,j] = kernel(training_fold[i,:-1], training_fold[j,:-1], sigma)\n",
        "      # Compute alpha vector\n",
        "      identity_matrix = np.identity(training_fold.shape[0])\n",
        "      alpha = np.matmul((np.linalg.inv(K_matrix + (gamma * training_fold.shape[0] * identity_matrix))), training_fold[:,-1])\n",
        "      # Compute error for every data point within the given fold\n",
        "      for i in range(validation_fold.shape[0]):\n",
        "        individual_terms = []\n",
        "        for j in range(training_fold.shape[0]):\n",
        "          individual_terms.append(alpha[j]*kernel(training_fold[j,:-1], validation_fold[i,:-1], sigma))\n",
        "        predicted_value = (sum(individual_terms))\n",
        "        individual_error = abs(predicted_value - validation_fold[i,-1])\n",
        "        error.append(individual_error)\n",
        "    # Compute average error for a given pairing of gamma and sigma\n",
        "    average_error = sum(error)/training_set_boston.shape[0]\n",
        "    error_matrix[g,s] = average_error\n",
        "\n",
        "error_df = pd.DataFrame(data=error_matrix, index=gamma_powers[:], columns=sigma_powers[:])\n",
        "error_df.to_csv('error_df_2.csv', sep=',')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbVDsQRNSZvp"
      },
      "source": [
        "# Determine minimum value of the mean error\n",
        "minimum = np.amin(error_matrix)\n",
        "\n",
        "# Identify the values of sigma and gamma that generated the minimum value:\n",
        "for i in range(error_matrix.shape[0]):\n",
        "  for j in range(error_matrix.shape[1]):\n",
        "    if error_matrix[i,j] == minimum:\n",
        "      best_gamma = gamma_values[i]\n",
        "      best_sigma = sigma_values[j]\n",
        "      print(f'Minimum mean error is achieved with gamma = 2^-{gamma_powers[i]}, and sigma = 2^{sigma_powers[j]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjSs3rmceYYr"
      },
      "source": [
        "minimum"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRW5AwUq4F3Z"
      },
      "source": [
        "error_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPR2CZQhO0gJ"
      },
      "source": [
        "#### (b) Plotting the cross-validation error"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biBp_oHaSFQ9"
      },
      "source": [
        "cross_validation_error = error_matrix/5\n",
        "\n",
        "cross_validation_error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGAS-GRhcrO0"
      },
      "source": [
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "X = np.array(sigma_powers)\n",
        "Y = np.array(gamma_powers)\n",
        "Z = cross_validation_error\n",
        "\n",
        "X_mesh, Y_mesh = np.meshgrid(X, Y)\n",
        "ax.set_xlabel('Sigma')\n",
        "ax.set_ylabel('Gamma')\n",
        "ax.set_zlabel('Cross-validation error')\n",
        "ax.dist = 11 # Avoid Z-axis label getting cut off\n",
        "ax.scatter(X_mesh, Y_mesh, Z)\n",
        "plt.show()\n",
        "fig.savefig(f\"q5b\", dpi=300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgZA7_kHlwfU"
      },
      "source": [
        "#### (c) MSE for best values of $\\gamma$ and $\\sigma$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpLlw8xmmFeW"
      },
      "source": [
        "gamma = best_gamma\n",
        "sigma = best_sigma\n",
        "# Compute Kernel matrix\n",
        "K_matrix = np.zeros((training_set_boston.shape[0], training_set_boston.shape[0]))\n",
        "for i in range(training_set_boston.shape[0]):\n",
        "    for j in range(training_set_boston.shape[0]):\n",
        "        K_matrix[i,j] = kernel(training_set_boston[i,:-1], training_set_boston[j,:-1], sigma)\n",
        "# Compute alpha vector\n",
        "identity_matrix = np.identity(training_set_boston.shape[0])\n",
        "alpha = np.matmul((np.linalg.inv(K_matrix + (gamma * training_set_boston.shape[0] * identity_matrix))), training_set_boston[:,-1])\n",
        "# Compute square error for data points from training set\n",
        "error_training = []\n",
        "for i in range(training_set_boston.shape[0]):\n",
        "  individual_terms = []\n",
        "  for j in range(training_set_boston.shape[0]):\n",
        "    individual_terms.append(alpha[j]*kernel(training_set_boston[j,:-1], training_set_boston[i,:-1], sigma))\n",
        "  predicted_value = (sum(individual_terms))\n",
        "  individual_square_error = (predicted_value - training_set_boston[i,-1])**2\n",
        "  error_training.append(individual_square_error)\n",
        "# Compute square error for data points from test set\n",
        "error_test=[]\n",
        "for i in range(test_set_boston.shape[0]):\n",
        "  individual_terms = []\n",
        "  for j in range(training_set_boston.shape[0]):\n",
        "    individual_terms.append(alpha[j]*kernel(training_set_boston[j,:-1], test_set_boston[i,:-1], sigma))\n",
        "  predicted_value = (sum(individual_terms))\n",
        "  individual_square_error = (predicted_value - test_set_boston[i,-1])**2\n",
        "  error_test.append(individual_square_error)\n",
        "# Compute mean square error\n",
        "average_error_training = sum(error_training)/training_set_boston.shape[0]\n",
        "print(f'MSE on training set is {round(average_error_training, 2)}')\n",
        "average_error_test = sum(error_test)/test_set_boston.shape[0]\n",
        "print(f'MSE on test set is {round(average_error_test, 2)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnK8kWGvto0K"
      },
      "source": [
        "we obtain the following results:\n",
        "* MSE on training set is approx. 7.12\n",
        "* MSE on test set is approx. 12.84"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9NwJI9IvdAl"
      },
      "source": [
        "#### (d) Comparison of all methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dholJLzYvcEJ"
      },
      "source": [
        "runs = []\n",
        "for i in range(1,20+1):\n",
        "  runs.append(f'Run {i}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uup1KYnawWF7"
      },
      "source": [
        "methods = ['Naive Regression']\n",
        "for i in range(1, 12+1):\n",
        "  methods.append(f'Linear regression (attribute {i})')\n",
        "methods.extend(('Linear regression (all attributes)', 'Kernel Ridge Regression'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lm42NeKJxQU-"
      },
      "source": [
        "train_MSE = pd.DataFrame(index=methods, columns=runs)\n",
        "test_MSE = pd.DataFrame(index=methods, columns=runs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEKq48aHxzfE"
      },
      "source": [
        "for r in range(20):\n",
        "  # Generate training and test sets, as well as arrays of output variable for each set\n",
        "  training_set_boston, test_set_boston, training_price_array, test_price_array = generate_samples(housedata)\n",
        "\n",
        "# Naive regression\n",
        "  # Create vectors of ones that are the same length as the training and testing sets, respectively\n",
        "  ones_training_set = np.ones(training_set_boston.shape[0])\n",
        "  ones_test_set = np.ones(test_set_boston.shape[0])\n",
        "\n",
        "  # Calculate single regressor parameter based on training set \n",
        "  regressor = (np.matmul(ones_training_set.T,ones_training_set))**(-1)*(np.matmul(ones_training_set.T,training_price_array))\n",
        "  \n",
        "  # Compute MSE on training set\n",
        "  mse_training_single_run = np.sum((training_price_array - (regressor*ones_training_set))**2)/training_set_boston.shape[0]\n",
        "  train_MSE.iloc[0,r] = mse_training_single_run\n",
        "\n",
        "  # Compute MSE on test set\n",
        "  mse_test_single_run = np.sum((test_price_array - (regressor*ones_test_set))**2)/test_set_boston.shape[0]\n",
        "  test_MSE.iloc[0,r] = mse_test_single_run\n",
        "\n",
        "# Linear regression based on single attribute\n",
        "  for k in range(0,12):\n",
        "\n",
        "  # Extract relevant input variable of training and test sets, and generate arrays\n",
        "    training_feature_array = training_set_boston[:,k]\n",
        "    test_feature_array = test_set_boston[:,k]\n",
        "  \n",
        "  # Add bias\n",
        "    ones_training_set = np.ones(training_set_boston.shape[0])\n",
        "    ones_test_set = np.ones(test_set_boston.shape[0])\n",
        "    training_matrix_single = np.stack((training_feature_array, ones_training_set), axis = 1)\n",
        "    test_matrix_single = np.stack((test_feature_array, ones_test_set), axis = 1)\n",
        "\n",
        "  # Calculate regressor vector based on training set \n",
        "    regressor_single = np.matmul((np.linalg.inv(np.matmul(training_matrix_single.T,training_matrix_single))),(np.matmul(training_matrix_single.T,training_price_array)))\n",
        "\n",
        "  # Compute MSE on training set\n",
        "    mse_training_single_run = np.sum((training_price_array - (np.matmul(training_matrix_single, regressor_single)))**2)/training_set_boston.shape[0]\n",
        "    train_MSE.iloc[k+1,r] = mse_training_single_run\n",
        "\n",
        "  # Compute MSE on test set\n",
        "    mse_test_single_run = np.sum((test_price_array - (np.matmul(test_matrix_single, regressor_single)))**2)/test_set_boston.shape[0]\n",
        "    test_MSE.iloc[k+1,r] = mse_test_single_run\n",
        "  \n",
        "# Linear regression based on all attributes\n",
        "  # Extract relevant input variables of training and test sets, and generate arrays\n",
        "  training_features_all = training_set_boston[:,:-1]\n",
        "  test_features_all = test_set_boston[:,:-1]\n",
        "  \n",
        "  # Add bias\n",
        "  ones_training_set = np.ones((training_set_boston.shape[0], 1))\n",
        "  ones_test_set = np.ones((test_set_boston.shape[0], 1))\n",
        "  training_matrix_all = np.hstack((training_features_all, ones_training_set))\n",
        "  test_matrix_all = np.hstack((test_features_all, ones_test_set))\n",
        "\n",
        "  # Calculate regressor vector based on training set \n",
        "  regressor_all = np.matmul((np.linalg.inv(np.matmul(training_matrix_all.T,training_matrix_all))),(np.matmul(training_matrix_all.T,training_price_array)))\n",
        "\n",
        "  # Compute MSE on training set\n",
        "  mse_training_single_run = np.sum((training_price_array - (np.matmul(training_matrix_all, regressor_all)))**2)/training_set_boston.shape[0]\n",
        "  train_MSE.iloc[13,r] = mse_training_single_run\n",
        "\n",
        "  # Compute MSE on test set\n",
        "  mse_test_single_run = np.sum((test_price_array - (np.matmul(test_matrix_all, regressor_all)))**2)/test_set_boston.shape[0]\n",
        "  test_MSE.iloc[13,r] = mse_test_single_run\n",
        "\n",
        "# Kernel Ridge Regression\n",
        "  \n",
        "  # Step 1: Identify best values of sigma and gamma \n",
        "  # Split training set into validation folds\n",
        "  validation_folds = gen_validation_folds(training_set_boston, 5)\n",
        "\n",
        "  error_matrix = np.zeros((len(gamma_values), len(sigma_values)))\n",
        "  for g in range(len(gamma_values)):\n",
        "    gamma = gamma_values[g]\n",
        "    gamma_power = gamma_powers[g]\n",
        "    for s in range(len(sigma_values)):\n",
        "      sigma = sigma_values[s]\n",
        "      sigma_power = sigma_powers[s]\n",
        "      error = []\n",
        "      for f in range(5):\n",
        "        training_fold = validation_folds[f][0]\n",
        "        validation_fold = validation_folds[f][1]\n",
        "        # Compute Kernel matrix\n",
        "        K_matrix = np.zeros((training_fold.shape[0], training_fold.shape[0]))\n",
        "        for i in range(training_fold.shape[0]):\n",
        "          for j in range(training_fold.shape[0]):\n",
        "            K_matrix[i,j] = kernel(training_fold[i,:-1], training_fold[j,:-1], sigma)\n",
        "        # Compute alpha vector\n",
        "        identity_matrix = np.identity(training_fold.shape[0])\n",
        "        alpha = np.matmul((np.linalg.inv(K_matrix + (gamma * training_fold.shape[0] * identity_matrix))), training_fold[:,-1])\n",
        "        # Compute error for every data point within the given fold\n",
        "        for i in range(validation_fold.shape[0]):\n",
        "          individual_terms = []\n",
        "          for j in range(training_fold.shape[0]):\n",
        "            individual_terms.append(alpha[j]*kernel(training_fold[j,:-1], validation_fold[i,:-1], sigma))\n",
        "          predicted_value = (sum(individual_terms))\n",
        "          individual_error = abs(predicted_value - validation_fold[i,-1])\n",
        "          error.append(individual_error)\n",
        "      # Compute average error for a given pairing of gamma and sigma\n",
        "      average_error = sum(error)/training_set_boston.shape[0]\n",
        "      error_matrix[g,s] = average_error\n",
        "  \n",
        "  # Determine minimum value of the mean error\n",
        "  minimum = np.amin(error_matrix)\n",
        "\n",
        "  # Identify the values of sigma and gamma that generated the minimum value:\n",
        "  for i in range(error_matrix.shape[0]):\n",
        "    for j in range(error_matrix.shape[1]):\n",
        "      if error_matrix[i,j] == minimum:\n",
        "        best_gamma = gamma_values[i]\n",
        "        best_sigma = sigma_values[j]\n",
        "\n",
        "  # Step 2: Apply Kernel Ridge Regression on training and test set using best values \n",
        "  gamma = best_gamma\n",
        "  sigma = best_sigma\n",
        "\n",
        "  # Compute Kernel matrix\n",
        "  K_matrix = np.zeros((training_set_boston.shape[0], training_set_boston.shape[0]))\n",
        "  for i in range(training_set_boston.shape[0]):\n",
        "      for j in range(training_set_boston.shape[0]):\n",
        "          K_matrix[i,j] = kernel(training_set_boston[i,:-1], training_set_boston[j,:-1], sigma)\n",
        "  # Compute alpha vector\n",
        "  identity_matrix = np.identity(training_set_boston.shape[0])\n",
        "  alpha = np.matmul((np.linalg.inv(K_matrix + (gamma * training_set_boston.shape[0] * identity_matrix))), training_set_boston[:,-1])\n",
        "  # Compute square error for data points from training set\n",
        "  error_training = []\n",
        "  for i in range(training_set_boston.shape[0]):\n",
        "    individual_terms = []\n",
        "    for j in range(training_set_boston.shape[0]):\n",
        "      individual_terms.append(alpha[j]*kernel(training_set_boston[j,:-1], training_set_boston[i,:-1], sigma))\n",
        "    predicted_value = (sum(individual_terms))\n",
        "    individual_square_error = (predicted_value - training_set_boston[i,-1])**2\n",
        "    error_training.append(individual_square_error)\n",
        "  mse_training_single_run = sum(error_training)/training_set_boston.shape[0]\n",
        "  train_MSE.iloc[14,r] = mse_training_single_run\n",
        "  # Compute square error for data points from test set\n",
        "  error_test=[]\n",
        "  for i in range(test_set_boston.shape[0]):\n",
        "    individual_terms = []\n",
        "    for j in range(training_set_boston.shape[0]):\n",
        "      individual_terms.append(alpha[j]*kernel(training_set_boston[j,:-1], test_set_boston[i,:-1], sigma))\n",
        "    predicted_value = (sum(individual_terms))\n",
        "    individual_square_error = (predicted_value - test_set_boston[i,-1])**2\n",
        "    error_test.append(individual_square_error)\n",
        "  mse_test_single_run = sum(error_test)/test_set_boston.shape[0]\n",
        "  test_MSE.iloc[14,r] = mse_test_single_run\n",
        "\n",
        "train_MSE.to_csv('MSE_train.csv', sep=',')\n",
        "test_MSE.to_csv('MSE_test.csv', sep=',')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCbLFFgatTom"
      },
      "source": [
        "train_MSE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaEBrgDfMd1p"
      },
      "source": [
        "summary = pd.DataFrame(index=methods, columns=('MSE train mean', 'MSE train std dev', 'MSE test mean', 'MSE test std dev'))\n",
        "\n",
        "for i in range(summary.shape[0]):\n",
        "  summary.iloc[i,0] = train_MSE.iloc[i,:].mean(axis=0)\n",
        "  summary.iloc[i,1] = train_MSE.iloc[i,:].std(axis=0)\n",
        "  summary.iloc[i,2] = test_MSE.iloc[i,:].mean(axis=0)\n",
        "  summary.iloc[i,3] = test_MSE.iloc[i,:].std(axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rInrpM5kmhH_"
      },
      "source": [
        "summary.to_csv('summary_ex5.csv', sep=',')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3L_eu6fBCOqp"
      },
      "source": [
        "summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhFbnW878fzE"
      },
      "source": [
        "test_MSE\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}