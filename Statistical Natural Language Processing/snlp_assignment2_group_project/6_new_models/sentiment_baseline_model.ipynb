{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"sentiment_baseline_model.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOQKK2LcleqzlphbrjKOCgR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"63so1L5veYM0"},"source":["# Imports"]},{"cell_type":"code","metadata":{"id":"ehicjqe6ot8z"},"source":["#installs\n","!pip install transformers\n","\n","#imports\n","import numpy as np\n","import torch\n","from transformers import pipeline\n","import os\n","import pandas as pd\n","import re\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YfB8NjZ5edlD"},"source":["# Colab setup"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6KXXmvXgpb9g","executionInfo":{"status":"ok","timestamp":1617202236944,"user_tz":-60,"elapsed":25057,"user":{"displayName":"Ravi Patel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhRJOsfZJS05r8g46ed_4acV4bLfM1kFbbxp-CqDU0=s64","userId":"04181830909770003963"}},"outputId":"587d491b-c399-41ed-af35-bafcd9771d94"},"source":["# mount google drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# change to directory containing relevant files\n","%cd 'INSERT_DIRECTORY'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n","/content/drive/My Drive/Machine_learning/UCL/Modules/NL/NLP_CW2\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZNTS3Relej9u"},"source":["# The sentiment model\n","\n","Here we will evalute HuggingFace's pretrained sentiment analysis model on the ETHICS utilitarianism task, without any additional training on the task."]},{"cell_type":"code","metadata":{"id":"1DgpyHqDrXs1"},"source":["# Load the sentiment analysis pipeline from HuggingFace\n","# The pretrained model used is this one: https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english\n","# It has been fine-tuned on the SST-2 (standford sentiment tree bank)\n","\n","senti_pipeline = pipeline(\"sentiment-analysis\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qpg_L6y5sUt5"},"source":["# for playing with sentiment model\n","\n","test_sentence = \"I went to a wine tasting event with my mom and sister yesterday.  The admission was fairly cheap for the amount of wine available to try.\"\n","senti_pipeline(test_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DhY1D8TvqjBi"},"source":["# function for loading all sentences from csv file\n","# Note that the returned 'labels' variable will be empty here\n","# The 'sentences' alternate in order between good and bad (first loaded sentence is good, 2nd bad, etc)\n","\n","def load_util_sentences(data_dir, split=\"test\"):\n","    path = os.path.join(data_dir, \"util_{}.csv\".format(split))\n","    df = pd.read_csv(path, header=None)\n","    sentences = []\n","    for i in range(df.shape[0]):\n","        sentences.append(df.iloc[i, 0])\n","        sentences.append(df.iloc[i, 1])\n","    labels = [-1 for _ in range(len(sentences))]\n","    return sentences, labels"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jdwO7eNFxGQP"},"source":["# load datasets\n","\n","sentences_easy, _ = load_util_sentences(\"test\") # load easy test dataset\n","sentences_hard, _ = load_util_sentences(\"test_hard\") # load hard test dataset\n","\n","both_datasets = [sentences_easy, sentences_hard]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z1e7IC8vs3sr"},"source":["# run experiment\n","\n","corrects = 0 # number of sentence pairs correctly classified\n","total = 0 # total number of sentence pairs\n","clear_cut = 0 # number of sentence pairs where classifications were opposite and correct\n","\n","for dataset_idx, sentences in enumerate(both_datasets):\n","    corrects = 0\n","    total = 0\n","    clear_cut = 0\n","    for sent_idx in range(0,len(sentences),2):\n","        output_good = senti_pipeline(sentences[sent_idx])\n","        output_bad = senti_pipeline(sentences[sent_idx+1])\n","\n","        if output_good[0]['label'] == 'POSITIVE' and output_bad[0]['label'] == 'NEGATIVE':\n","            corrects += 1\n","            clear_cut += 1\n","        elif output_good[0]['label'] == 'NEGATIVE' and output_bad[0]['label'] == 'NEGATIVE' and output_good[0]['score'] < output_bad[0]['score']:\n","            corrects += 1\n","        elif output_good[0]['label'] == 'POSITIVE' and output_bad[0]['label'] == 'POSITIVE' and output_good[0]['score'] > output_bad[0]['score']:\n","            corrects += 1\n","\n","        total += 1\n","\n","    accuracy = (corrects / total)*100\n","    clear_cut_percentage = (clear_cut / total)*100\n","\n","    # print results\n","    if dataset_idx == 0:\n","        print(\"\\nEASY DATASET\")\n","        print(f\"Sentiment model's accuracy: {accuracy:.2f}%\")\n","        print(f\"Percentage of sentence pairs where sentiment evaluations were opposite and correct (i.e. clear cut): {clear_cut_percentage:.2f}%\")\n","    elif dataset_idx == 1:\n","        print(\"\\nHARD DATASET\")\n","        print(f\"Sentiment model's accuracy: {accuracy:.2f}%\")\n","        print(f\"Percentage of sentence pairs where sentiment evaluations were opposite and correct (i.e. clear cut): {clear_cut_percentage:.2f}%\")"],"execution_count":null,"outputs":[]}]}